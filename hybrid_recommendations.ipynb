{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hybrid_recommendations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkrissada/google_ml_training/blob/master/hybrid_recommendations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLC1DIX8xn87",
        "colab_type": "text"
      },
      "source": [
        "# Neural network hybrid recommendation system on Google Analytics data model and training\n",
        "\n",
        "This notebook demonstrates how to implement a hybrid recommendation system using a neural network to combine content-based and collaborative filtering recommendation models using Google Analytics data. We are going to use the learned user embeddings from [wals.ipynb](../wals.ipynb) and combine that with our previous content-based features from [content_based_using_neural_networks.ipynb](../content_based_using_neural_networks.ipynb)\n",
        "\n",
        "Now that we have our data preprocessed from BigQuery and Cloud Dataflow, we can build our neural network hybrid recommendation model to our preprocessed data. Then we can train locally to make sure everything works and then use the power of Google Cloud ML Engine to scale it out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1ZofZe5xn88",
        "colab_type": "text"
      },
      "source": [
        "We're going to use TensorFlow Hub to use trained text embeddings, so let's first pip install that and reset our session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyp4MsMGxn89",
        "colab_type": "code",
        "colab": {},
        "outputId": "6a3d854c-29c5-4a39-b7ad-9d2eab8c5c9a"
      },
      "source": [
        "!pip3 install tensorflow_hub"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/envs/py3env/lib/python3.5/site-packages (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub) (3.6.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub) (1.10.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.4.0->tensorflow_hub) (40.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4IghrsIxn9D",
        "colab_type": "code",
        "colab": {},
        "outputId": "0299a66f-8a18-4f63-b81e-9d9caac87741"
      },
      "source": [
        "%%bash\n",
        "pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/envs/py3env/lib/python3.5/site-packages (1.13.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<1.14.0,>=1.13.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (3.6.1)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (0.31.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow) (1.17.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (2.6.11)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.6.1->tensorflow) (40.2.0)\n",
            "Requirement already satisfied, skipping upgrade: mock>=2.0.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pbr>=0.11 in /usr/local/envs/py3env/lib/python3.5/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (4.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_suwcK1dxn9G",
        "colab_type": "text"
      },
      "source": [
        "Now reset the notebook's session kernel! Since we're no longer using Cloud Dataflow, we'll be using the python3 kernel from here on out so don't forget to change the kernel if it's still python2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoDxJqXzxn9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import helpful libraries and setup our project, bucket, and region\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# PROJECT = \"cloud-training-demos\" # REPLACE WITH YOUR PROJECT ID\n",
        "# BUCKET = \"cloud-training-demos-ml\" # REPLACE WITH YOUR BUCKET NAME\n",
        "# REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
        "PROJECT = \"qwiklabs-gcp-d54a1ed2fe64d873\" # REPLACE WITH YOUR PROJECT ID\n",
        "BUCKET = \"qwiklabs-gcp-d54a1ed2fe64d873\" # REPLACE WITH YOUR BUCKET NAME\n",
        "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
        "\n",
        "# do not change these\n",
        "os.environ[\"PROJECT\"] = PROJECT\n",
        "os.environ[\"BUCKET\"] = BUCKET\n",
        "os.environ[\"REGION\"] = REGION\n",
        "os.environ[\"TFVERSION\"] = \"1.13\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKqLTMU_xn9I",
        "colab_type": "code",
        "colab": {},
        "outputId": "4decf35c-25f3-46a7-ef1c-9b1d7a1bb3a9"
      },
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-mGSATwxn9M",
        "colab_type": "code",
        "colab": {},
        "outputId": "5ffbcd8f-3e1b-49e5-ccfb-d520cbb4ab31"
      },
      "source": [
        "%%bash\n",
        "if ! gsutil ls | grep -q gs://${BUCKET}/hybrid_recommendation/preproc; then\n",
        "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
        "    # copy canonical set of preprocessed files if you didn't do preprocessing notebook\n",
        "    gsutil -m cp -R gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation gs://${BUCKET}\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating gs://qwiklabs-gcp-d54a1ed2fe64d873/...\n",
            "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/eval.csv-00000-of-00001 [Content-Type=text/plain]...\n",
            "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/tmp/staging/preprocess-hybrid-recommendation-features-181217-164834.1545065316.946936/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/ 11.4 MiB]                                                \r/ [0 files][    0.0 B/ 13.6 MiB]                                                \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/tmp/staging/preprocess-hybrid-recommendation-features-181217-164834.1545065316.946936/dataflow_python_sdk.tar [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/ 16.0 MiB]                                                \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/tmp/staging/preprocess-hybrid-recommendation-features-181217-164834.1545065316.946936/pipeline.pb [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/ 16.1 MiB]                                                \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/train.csv-00000-of-00004 [Content-Type=text/plain]...\n",
            "/ [0 files][    0.0 B/ 18.0 MiB]                                                \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/train.csv-00001-of-00004 [Content-Type=text/plain]...\n",
            "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/train.csv-00002-of-00004 [Content-Type=text/plain]...\n",
            "/ [1/21 files][ 41.3 KiB/128.8 MiB]   0% Done                                   \r/ [1/21 files][ 41.3 KiB/128.8 MiB]   0% Done                                   \r/ [2/21 files][  2.4 MiB/128.8 MiB]   1% Done                                   \r/ [2/21 files][  2.4 MiB/128.8 MiB]   1% Done                                   \r-\r- [3/21 files][  4.4 MiB/128.8 MiB]   3% Done                                   \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/features/train.csv-00003-of-00004 [Content-Type=text/plain]...\n",
            "- [3/21 files][  4.4 MiB/128.8 MiB]   3% Done                                   \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/author_vocab_count.txt-00000-of-00001 [Content-Type=text/plain]...\n",
            "Copying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/category_vocab_count.txt-00000-of-00001 [Content-Type=text/plain]...\n",
            "- [4/21 files][  6.7 MiB/128.8 MiB]   5% Done                                   \r- [4/21 files][  6.7 MiB/128.8 MiB]   5% Done                                   \r- [5/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r- [5/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/content_id_vocab_count.txt-00000-of-00001 [Content-Type=text/plain]...\n",
            "\\\r\\ [6/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r\\ [6/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \r\\ [7/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/months_since_epoch_mean.txt-00000-of-00001 [Content-Type=text/plain]...\n",
            "\\ [7/21 files][ 18.0 MiB/128.8 MiB]  14% Done                                   \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/tmp/staging/preprocess-hybrid-recommendation-vocab-counts-181217-170736.1545066458.457255/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl [Content-Type=application/octet-stream]...\n",
            "\\ [8/21 files][ 39.3 MiB/128.8 MiB]  30% Done                                   \r\\ [8/21 files][ 39.3 MiB/128.8 MiB]  30% Done                                   \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/tmp/staging/preprocess-hybrid-recommendation-vocab-counts-181217-170736.1545066458.457255/dataflow_python_sdk.tar [Content-Type=application/octet-stream]...\n",
            "\\ [9/21 files][ 39.3 MiB/128.8 MiB]  30% Done                                   \r\\ [9/21 files][ 39.3 MiB/128.8 MiB]  30% Done                                   \r\\ [10/21 files][ 68.6 MiB/128.8 MiB]  53% Done                                  \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocab_counts/tmp/staging/preprocess-hybrid-recommendation-vocab-counts-181217-170736.1545066458.457255/pipeline.pb [Content-Type=application/octet-stream]...\n",
            "\\ [10/21 files][ 68.6 MiB/128.8 MiB]  53% Done                                  \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/author_vocab.txt-00000-of-00001 [Content-Type=text/plain]...\n",
            "\\ [11/21 files][ 68.6 MiB/128.8 MiB]  53% Done                                  \r\\ [11/21 files][ 68.6 MiB/128.8 MiB]  53% Done                                  \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/category_vocab.txt-00000-of-00001 [Content-Type=text/plain]...\n",
            "|\r| [12/21 files][ 70.9 MiB/128.8 MiB]  55% Done                                  \r| [12/21 files][ 70.9 MiB/128.8 MiB]  55% Done                                  \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/content_id_vocab.txt-00000-of-00001 [Content-Type=text/plain]...\n",
            "| [13/21 files][ 71.0 MiB/128.8 MiB]  55% Done                                  \r| [13/21 files][ 71.0 MiB/128.8 MiB]  55% Done                                  \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/tmp/staging/preprocess-hybrid-recommendation-vocab-lists-181217-170024.1545066026.063994/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl [Content-Type=application/octet-stream]...\n",
            "| [14/21 files][ 71.0 MiB/128.8 MiB]  55% Done                                  \r| [14/21 files][ 71.0 MiB/128.8 MiB]  55% Done                                  \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/tmp/staging/preprocess-hybrid-recommendation-vocab-lists-181217-170024.1545066026.063994/dataflow_python_sdk.tar [Content-Type=application/octet-stream]...\n",
            "| [15/21 files][ 73.4 MiB/128.8 MiB]  56% Done                                  \r| [15/21 files][ 73.4 MiB/128.8 MiB]  56% Done                                  \rCopying gs://cloud-training-demos/courses/machine_learning/deepdive/10_recommendation/hybrid_recommendation/preproc/vocabs/tmp/staging/preprocess-hybrid-recommendation-vocab-lists-181217-170024.1545066026.063994/pipeline.pb [Content-Type=application/octet-stream]...\n",
            "| [16/21 files][124.0 MiB/128.8 MiB]  96% Done                                  \r| [16/21 files][124.0 MiB/128.8 MiB]  96% Done                                  \r| [17/21 files][124.0 MiB/128.8 MiB]  96% Done                                  \r/\r/ [18/21 files][124.1 MiB/128.8 MiB]  96% Done                                  \r/ [19/21 files][126.4 MiB/128.8 MiB]  98% Done                                  \r/ [20/21 files][126.4 MiB/128.8 MiB]  98% Done                                  \r/ [21/21 files][128.8 MiB/128.8 MiB] 100% Done                                  \r\n",
            "Operation completed over 21 objects/128.8 MiB.                                   \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1xJLmIYxn9O",
        "colab_type": "text"
      },
      "source": [
        "<h2> Create hybrid recommendation system model using TensorFlow </h2>\n",
        "\n",
        "Now that we've created our training and evaluation input files as well as our categorical feature vocabulary files, we can create our TensorFlow hybrid recommendation system model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVtrMtCrxn9P",
        "colab_type": "text"
      },
      "source": [
        "Let's first get some of our aggregate information that we will use in the model from some of our preprocessed files we saved in Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmMwuta2xn9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.lib.io import file_io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPJlWCqpxn9S",
        "colab_type": "code",
        "colab": {},
        "outputId": "f2d793e8-4c92-4b64-ef42-1bad92004edf"
      },
      "source": [
        "# Get number of content ids from text file in Google Cloud Storage\n",
        "with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocab_counts/content_id_vocab_count.txt*\".format(BUCKET))[0], mode = 'r') as ifp:\n",
        "    number_of_content_ids = int([x for x in ifp][0])\n",
        "print(\"number_of_content_ids = {}\".format(number_of_content_ids))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number_of_content_ids = 15634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jhOmB41xn9V",
        "colab_type": "code",
        "colab": {},
        "outputId": "f7b8db04-e436-4c08-f363-10f88a0e3f55"
      },
      "source": [
        "# Get number of categories from text file in Google Cloud Storage\n",
        "with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocab_counts/category_vocab_count.txt*\".format(BUCKET))[0], mode = 'r') as ifp:\n",
        "    number_of_categories = int([x for x in ifp][0])\n",
        "print(\"number_of_categories = {}\".format(number_of_categories))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number_of_categories = 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDvhBbXxn9Z",
        "colab_type": "code",
        "colab": {},
        "outputId": "d37fc3af-1bb3-44e7-b56e-1f3832079215"
      },
      "source": [
        "# Get number of authors from text file in Google Cloud Storage\n",
        "with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocab_counts/author_vocab_count.txt*\".format(BUCKET))[0], mode = 'r') as ifp:\n",
        "    number_of_authors = int([x for x in ifp][0])\n",
        "print(\"number_of_authors = {}\".format(number_of_authors))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number_of_authors = 1103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6a0pFA9xn9d",
        "colab_type": "code",
        "colab": {},
        "outputId": "51fe5b3d-6719-4957-e6f0-e3427c3c32e7"
      },
      "source": [
        "# Get mean months since epoch from text file in Google Cloud Storage\n",
        "with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocab_counts/months_since_epoch_mean.txt*\".format(BUCKET))[0], mode = 'r') as ifp:\n",
        "    mean_months_since_epoch = float([x for x in ifp][0])\n",
        "print(\"mean_months_since_epoch = {}\".format(mean_months_since_epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_months_since_epoch = 573.60733908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh-FPMQmxn9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Determine CSV and label columns\n",
        "NON_FACTOR_COLUMNS = \"next_content_id,visitor_id,content_id,category,title,author,months_since_epoch\".split(',')\n",
        "FACTOR_COLUMNS = [\"user_factor_{}\".format(i) for i in range(10)] + [\"item_factor_{}\".format(i) for i in range(10)]\n",
        "CSV_COLUMNS = NON_FACTOR_COLUMNS + FACTOR_COLUMNS\n",
        "LABEL_COLUMN = \"next_content_id\"\n",
        "\n",
        "# Set default values for each CSV column\n",
        "NON_FACTOR_DEFAULTS = [[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[mean_months_since_epoch]]\n",
        "FACTOR_DEFAULTS = [[0.0] for i in range(10)] + [[0.0] for i in range(10)] # user and item\n",
        "DEFAULTS = NON_FACTOR_DEFAULTS + FACTOR_DEFAULTS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3qbUNR4xn9j",
        "colab_type": "text"
      },
      "source": [
        "Create input function for training and evaluation to read from our preprocessed CSV files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPeHP_25xn9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create input function for train and eval\n",
        "def read_dataset(filename, mode, batch_size = 512):\n",
        "    def _input_fn():\n",
        "        def decode_csv(value_column):\n",
        "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS)\n",
        "            features = dict(zip(CSV_COLUMNS, columns))          \n",
        "            label = features.pop(LABEL_COLUMN)         \n",
        "            return features, label\n",
        "\n",
        "        # Create list of files that match pattern\n",
        "        file_list = tf.gfile.Glob(filename = filename)\n",
        "\n",
        "        # Create dataset from file list\n",
        "        dataset = tf.data.TextLineDataset(filenames = file_list).map(map_func = decode_csv)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None # indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this\n",
        "\n",
        "        dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "    return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqKlCvzixn9o",
        "colab_type": "text"
      },
      "source": [
        "Next, we will create our feature columns using our read in features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW_7pCymxn9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create feature columns to be used in model\n",
        "def create_feature_columns(args):\n",
        "    # Create content_id feature column\n",
        "    content_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "        key = \"content_id\",\n",
        "        hash_bucket_size = number_of_content_ids)\n",
        "\n",
        "    # Embed content id into a lower dimensional representation\n",
        "    embedded_content_column = tf.feature_column.embedding_column(\n",
        "        categorical_column = content_id_column,\n",
        "        dimension = args[\"content_id_embedding_dimensions\"])\n",
        "\n",
        "    # Create category feature column\n",
        "    categorical_category_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
        "        key = \"category\",\n",
        "        vocabulary_file = tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocabs/category_vocab.txt*\".format(args[\"bucket\"]))[0],\n",
        "        num_oov_buckets = 1)\n",
        "\n",
        "    # Convert categorical category column into indicator column so that it can be used in a DNN\n",
        "    indicator_category_column = tf.feature_column.indicator_column(categorical_column = categorical_category_column)\n",
        "\n",
        "    # Create title feature column using TF Hub\n",
        "    embedded_title_column = hub.text_embedding_column(\n",
        "        key = \"title\", \n",
        "        module_spec = \"https://tfhub.dev/google/nnlm-de-dim50-with-normalization/1\",\n",
        "        trainable = False)\n",
        "\n",
        "    # Create author feature column\n",
        "    author_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "        key = \"author\",\n",
        "        hash_bucket_size = number_of_authors + 1)\n",
        "\n",
        "    # Embed author into a lower dimensional representation\n",
        "    embedded_author_column = tf.feature_column.embedding_column(\n",
        "        categorical_column = author_column,\n",
        "        dimension = args[\"author_embedding_dimensions\"])\n",
        "\n",
        "    # Create months since epoch boundaries list for our binning\n",
        "    months_since_epoch_boundaries = list(range(400, 700, 20))\n",
        "\n",
        "    # Create months_since_epoch feature column using raw data\n",
        "    months_since_epoch_column = tf.feature_column.numeric_column(\n",
        "        key = \"months_since_epoch\")\n",
        "\n",
        "    # Create bucketized months_since_epoch feature column using our boundaries\n",
        "    months_since_epoch_bucketized = tf.feature_column.bucketized_column(\n",
        "        source_column = months_since_epoch_column,\n",
        "        boundaries = months_since_epoch_boundaries)\n",
        "\n",
        "    # Cross our categorical category column and bucketized months since epoch column\n",
        "    crossed_months_since_category_column = tf.feature_column.crossed_column(\n",
        "        keys = [categorical_category_column, months_since_epoch_bucketized],\n",
        "        hash_bucket_size = len(months_since_epoch_boundaries) * (number_of_categories + 1))\n",
        "\n",
        "    # Convert crossed categorical category and bucketized months since epoch column into indicator column so that it can be used in a DNN\n",
        "    indicator_crossed_months_since_category_column = tf.feature_column.indicator_column(\n",
        "            categorical_column = crossed_months_since_category_column)\n",
        "\n",
        "    # Create user and item factor feature columns from our trained WALS model\n",
        "    user_factors = [tf.feature_column.numeric_column(key = \"user_factor_\" + str(i)) for i in range(10)]\n",
        "    item_factors =  [tf.feature_column.numeric_column(key = \"item_factor_\" + str(i)) for i in range(10)]\n",
        "\n",
        "    # Create list of feature columns\n",
        "    feature_columns = [embedded_content_column,\n",
        "    embedded_author_column,\n",
        "    indicator_category_column,\n",
        "    embedded_title_column,\n",
        "    indicator_crossed_months_since_category_column] + user_factors + item_factors\n",
        "\n",
        "    return feature_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNGeq176xn9s",
        "colab_type": "text"
      },
      "source": [
        "Now we'll create our model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19KsCen1xn9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create custom model function for our custom estimator\n",
        "def model_fn(features, labels, mode, params):\n",
        "    # Create neural network input layer using our feature columns defined above\n",
        "    net = tf.feature_column.input_layer(features = features, feature_columns = params[\"feature_columns\"])\n",
        "\n",
        "    # Create hidden layers by looping through hidden unit list\n",
        "    for units in params[\"hidden_units\"]:\n",
        "        net = tf.layers.dense(inputs = net, units = units, activation = tf.nn.relu)\n",
        "\n",
        "    # Compute logits (1 per class) using the output of our last hidden layer\n",
        "    logits = tf.layers.dense(inputs = net, units = params[\"n_classes\"], activation = None)\n",
        "\n",
        "    # Find the predicted class indices based on the highest logit (which will result in the highest probability)\n",
        "    predicted_classes = tf.argmax(input = logits, axis = 1)\n",
        "\n",
        "  # Read in the content id vocabulary so we can tie the predicted class indices to their respective content ids\n",
        "    with file_io.FileIO(tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocabs/content_id_vocab.txt*\".format(BUCKET))[0], mode = \"r\") as ifp:\n",
        "        content_id_names = tf.constant(value = [x.rstrip() for x in ifp])\n",
        "\n",
        "    # Gather predicted class names based predicted class indices\n",
        "    predicted_class_names = tf.gather(params = content_id_names, indices = predicted_classes)\n",
        "\n",
        "    # If the mode is prediction\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        # Create predictions dict\n",
        "        predictions_dict = {\n",
        "            \"class_ids\": tf.expand_dims(input = predicted_classes, axis = -1),\n",
        "            \"class_names\" : tf.expand_dims(input = predicted_class_names, axis = -1),\n",
        "            \"probabilities\": tf.nn.softmax(logits = logits),\n",
        "            \"logits\": logits\n",
        "        }\n",
        "\n",
        "        # Create export outputs\n",
        "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
        "\n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for prediction mode\n",
        "          mode = mode,\n",
        "          predictions = predictions_dict,\n",
        "          loss = None,\n",
        "          train_op = None,\n",
        "          eval_metric_ops = None,\n",
        "          export_outputs = export_outputs)\n",
        "\n",
        "    # Continue on with training and evaluation modes\n",
        "\n",
        "    # Create lookup table using our content id vocabulary\n",
        "    table = tf.contrib.lookup.index_table_from_file(\n",
        "        vocabulary_file = tf.gfile.Glob(filename = \"gs://{}/hybrid_recommendation/preproc/vocabs/content_id_vocab.txt*\".format(BUCKET))[0])\n",
        "\n",
        "    # Look up labels from vocabulary table\n",
        "    labels = table.lookup(keys = labels)\n",
        "\n",
        "    # Compute loss using sparse softmax cross entropy since this is classification and our labels (content id indices) and probabilities are mutually exclusive\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits = logits)\n",
        "\n",
        "    # If the mode is evaluation\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        # Compute evaluation metrics of total accuracy and the accuracy of the top k classes\n",
        "        accuracy = tf.metrics.accuracy(labels = labels, predictions = predicted_classes, name = \"acc_op\")\n",
        "        top_k_accuracy = tf.metrics.mean(values = tf.nn.in_top_k(predictions = logits, targets = labels, k = params[\"top_k\"]))\n",
        "        map_at_k = tf.metrics.average_precision_at_k(labels = labels, predictions = predicted_classes, k = params[\"top_k\"])\n",
        "\n",
        "        # Put eval metrics into a dictionary\n",
        "        eval_metric_ops = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"top_k_accuracy\": top_k_accuracy,\n",
        "            \"map_at_k\": map_at_k}\n",
        "\n",
        "        # Create scalar summaries to see in TensorBoard\n",
        "        tf.summary.scalar(name = \"accuracy\", tensor = accuracy[1])\n",
        "        tf.summary.scalar(name = \"top_k_accuracy\", tensor = top_k_accuracy[1])\n",
        "        tf.summary.scalar(name = \"map_at_k\", tensor = map_at_k[1])\n",
        "        \n",
        "        return tf.estimator.EstimatorSpec( # return early since we\"re done with what we need for evaluation mode\n",
        "            mode = mode,\n",
        "            predictions = None,\n",
        "            loss = loss,\n",
        "            train_op = None,\n",
        "            eval_metric_ops = eval_metric_ops,\n",
        "            export_outputs = None)\n",
        "\n",
        "    # Continue on with training mode\n",
        "\n",
        "    # If the mode is training\n",
        "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "    # Create a custom optimizer\n",
        "    optimizer = tf.train.AdagradOptimizer(learning_rate = params[\"learning_rate\"])\n",
        "\n",
        "    # Create train op\n",
        "    train_op = optimizer.minimize(loss = loss, global_step = tf.train.get_global_step())\n",
        "\n",
        "    return tf.estimator.EstimatorSpec( # final return since we\"re done with what we need for training mode\n",
        "        mode = mode,\n",
        "        predictions = None,\n",
        "        loss = loss,\n",
        "        train_op = train_op,\n",
        "        eval_metric_ops = None,\n",
        "        export_outputs = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRjlSjztxn9v",
        "colab_type": "text"
      },
      "source": [
        "Now create a serving input function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11HVhLehxn9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create serving input function\n",
        "def serving_input_fn():  \n",
        "    feature_placeholders = {\n",
        "        colname : tf.placeholder(dtype = tf.string, shape = [None]) \\\n",
        "        for colname in NON_FACTOR_COLUMNS[1:-1]\n",
        "    }\n",
        "    feature_placeholders[\"months_since_epoch\"] = tf.placeholder(dtype = tf.float32, shape = [None])\n",
        "\n",
        "    for colname in FACTOR_COLUMNS:\n",
        "        feature_placeholders[colname] = tf.placeholder(dtype = tf.float32, shape = [None])\n",
        "\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1) \\\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhD6eV36xn9x",
        "colab_type": "text"
      },
      "source": [
        "Now that all of the pieces are assembled let's create and run our train and evaluate loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQJem2Ynxn9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create train and evaluate loop to combine all of the pieces together.\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "def train_and_evaluate(args):\n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn = model_fn,\n",
        "        model_dir = args[\"output_dir\"],\n",
        "        params = {\n",
        "        \"feature_columns\": create_feature_columns(args),\n",
        "        \"hidden_units\": args[\"hidden_units\"],\n",
        "        \"n_classes\": number_of_content_ids,\n",
        "        \"learning_rate\": args[\"learning_rate\"],\n",
        "        \"top_k\": args[\"top_k\"],\n",
        "        \"bucket\": args[\"bucket\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn = read_dataset(filename = args[\"train_data_paths\"], mode = tf.estimator.ModeKeys.TRAIN, batch_size = args[\"batch_size\"]),\n",
        "        max_steps = args[\"train_steps\"])\n",
        "\n",
        "    exporter = tf.estimator.LatestExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_fn)\n",
        "\n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "        input_fn = read_dataset(filename = args[\"eval_data_paths\"], mode = tf.estimator.ModeKeys.EVAL, batch_size = args[\"batch_size\"]),\n",
        "        steps = None,\n",
        "        start_delay_secs = args[\"start_delay_secs\"],\n",
        "        throttle_secs = args[\"throttle_secs\"],\n",
        "        exporters = exporter)\n",
        "\n",
        "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYt1PmHqxn90",
        "colab_type": "text"
      },
      "source": [
        "Run train_and_evaluate!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "231PoMqwxn90",
        "colab_type": "code",
        "colab": {},
        "outputId": "09869582-fb9e-4bba-9bbd-86936e6b5743"
      },
      "source": [
        "# Call train and evaluate loop\n",
        "import shutil\n",
        "\n",
        "outdir = \"hybrid_recommendation_trained\"\n",
        "shutil.rmtree(path = outdir, ignore_errors = True) # start fresh each time\n",
        "\n",
        "arguments = {\n",
        "    \"bucket\": BUCKET,\n",
        "    \"train_data_paths\": \"gs://{}/hybrid_recommendation/preproc/features/train.csv*\".format(BUCKET),\n",
        "    \"eval_data_paths\": \"gs://{}/hybrid_recommendation/preproc/features/eval.csv*\".format(BUCKET),\n",
        "    \"output_dir\": outdir,\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"hidden_units\": [256, 128, 64],\n",
        "    \"content_id_embedding_dimensions\": 10,\n",
        "    \"author_embedding_dimensions\": 10,\n",
        "    \"top_k\": 10,\n",
        "    \"train_steps\": 1000,\n",
        "    \"start_delay_secs\": 30,\n",
        "    \"throttle_secs\": 30\n",
        "}\n",
        "\n",
        "train_and_evaluate(arguments)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:vocabulary_size = 3 in category is inferred from the number of elements in the vocabulary_file gs://qwiklabs-gcp-d54a1ed2fe64d873/hybrid_recommendation/preproc/vocabs/category_vocab.txt-00000-of-00001.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:52:52.445419 140200235099904 feature_column_v2.py:1625] vocabulary_size = 3 in category is inferred from the number of elements in the vocabulary_file gs://qwiklabs-gcp-d54a1ed2fe64d873/hybrid_recommendation/preproc/vocabs/category_vocab.txt-00000-of-00001.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:22.992029 140200235099904 estimator.py:1739] Using default config.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_task_id': 0, '_model_dir': 'hybrid_recommendation_trained', '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_master': '', '_log_step_count_steps': 100, '_save_summary_steps': 100, '_eval_distribute': None, '_num_ps_replicas': 0, '_experimental_distribute': None, '_save_checkpoints_steps': None, '_protocol': None, '_train_distribute': None, '_evaluation_master': '', '_global_id_in_cluster': 0, '_num_worker_replicas': 1, '_device_fn': None, '_is_chief': True, '_task_type': 'worker', '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f82a2fe66a0>}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:23.021491 140200235099904 estimator.py:201] Using config: {'_task_id': 0, '_model_dir': 'hybrid_recommendation_trained', '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_master': '', '_log_step_count_steps': 100, '_save_summary_steps': 100, '_eval_distribute': None, '_num_ps_replicas': 0, '_experimental_distribute': None, '_save_checkpoints_steps': None, '_protocol': None, '_train_distribute': None, '_evaluation_master': '', '_global_id_in_cluster': 0, '_num_worker_replicas': 1, '_device_fn': None, '_is_chief': True, '_task_type': 'worker', '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f82a2fe66a0>}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not using Distribute Coordinator.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:23.032705 140200235099904 estimator_training.py:185] Not using Distribute Coordinator.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:23.037605 140200235099904 training.py:610] Running training and evaluation locally (non-distributed).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:23.041005 140200235099904 training.py:698] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.075108 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:23.325376 140200235099904 estimator.py:1111] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: EmbeddingColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.334492 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: EmbeddingColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3100: HashedCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.342680 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3100: HashedCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: HashedCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.345519 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: HashedCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3040: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.364204 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3040: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:206: EmbeddingColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.431527 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:206: EmbeddingColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: IndicatorColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.447360 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: IndicatorColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.454761 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: CrossedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.458384 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: CrossedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: CrossedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.460149 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: CrossedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4115: VocabularyFileCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.463267 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4115: VocabularyFileCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: VocabularyFileCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.465841 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: VocabularyFileCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4115: BucketizedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.507817 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4115: BucketizedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: BucketizedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.515704 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: BucketizedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.518620 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.527299 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2898: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.565364 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2898: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.581118 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.588698 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: VocabularyFileCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.611190 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: VocabularyFileCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.705620 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.717336 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:23.828721 140200235099904 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:23.888029 140200235099904 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-21-a83e8c390d82>:8: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:23.990691 140200235099904 deprecation.py:323] From <ipython-input-21-a83e8c390d82>:8: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:53:27.641224 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:27.761714 140200235099904 estimator.py:1113] Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:27.772295 140200235099904 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:28.634097 140200235099904 monitored_session.py:222] Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:29.081258 140200235099904 session_manager.py:491] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:30.626458 140200235099904 session_manager.py:493] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into hybrid_recommendation_trained/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:31.519102 140200235099904 basic_session_run_hooks.py:594] Saving checkpoints for 0 into hybrid_recommendation_trained/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 9.656681, step = 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:33.203202 140200235099904 basic_session_run_hooks.py:249] loss = 9.656681, step = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 8.13911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:45.489241 140200235099904 basic_session_run_hooks.py:680] global_step/sec: 8.13911\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.378806, step = 101 (12.301 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:45.503906 140200235099904 basic_session_run_hooks.py:247] loss = 5.378806, step = 101 (12.301 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 8.53595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:57.204404 140200235099904 basic_session_run_hooks.py:680] global_step/sec: 8.53595\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.375754, step = 201 (11.712 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:53:57.216357 140200235099904 basic_session_run_hooks.py:247] loss = 4.375754, step = 201 (11.712 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 8.7093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:08.686373 140200235099904 basic_session_run_hooks.py:680] global_step/sec: 8.7093\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.790354, step = 301 (11.481 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:08.697651 140200235099904 basic_session_run_hooks.py:247] loss = 4.790354, step = 301 (11.481 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 9.04668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:19.740157 140200235099904 basic_session_run_hooks.py:680] global_step/sec: 9.04668\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.121499, step = 401 (11.054 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:19.751897 140200235099904 basic_session_run_hooks.py:247] loss = 5.121499, step = 401 (11.054 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 9.06497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:30.771639 140200235099904 basic_session_run_hooks.py:680] global_step/sec: 9.06497\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.6208563, step = 501 (11.031 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:30.782983 140200235099904 basic_session_run_hooks.py:247] loss = 4.6208563, step = 501 (11.031 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 9.10653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:41.752774 140200235099904 basic_session_run_hooks.py:680] global_step/sec: 9.10653\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.603697, step = 601 (10.982 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:41.764964 140200235099904 basic_session_run_hooks.py:247] loss = 4.603697, step = 601 (10.982 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 8.82413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:53.085345 140200235099904 basic_session_run_hooks.py:680] global_step/sec: 8.82413\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.044692, step = 701 (11.332 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:54:53.096912 140200235099904 basic_session_run_hooks.py:247] loss = 5.044692, step = 701 (11.332 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 9.07482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:04.104972 140200235099904 basic_session_run_hooks.py:680] global_step/sec: 9.07482\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 4.4556427, step = 801 (11.017 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:04.113814 140200235099904 basic_session_run_hooks.py:247] loss = 4.4556427, step = 801 (11.017 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 8.88594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:15.358545 140200235099904 basic_session_run_hooks.py:680] global_step/sec: 8.88594\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 5.346167, step = 901 (11.255 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:15.368909 140200235099904 basic_session_run_hooks.py:247] loss = 5.346167, step = 901 (11.255 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 1000 into hybrid_recommendation_trained/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:26.716445 140200235099904 basic_session_run_hooks.py:594] Saving checkpoints for 1000 into hybrid_recommendation_trained/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:27.632009 140200235099904 estimator.py:1111] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:27.962472 140200235099904 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:28.027020 140200235099904 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:2295: to_double (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:55:28.628957 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:2295: to_double (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:3040: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:55:28.643571 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:3040: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:28.692947 140200235099904 estimator.py:1113] Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-05-08T13:55:28Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:28.729183 140200235099904 evaluation.py:257] Starting evaluation at 2019-05-08T13:55:28Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:29.117606 140200235099904 monitored_session.py:222] Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:55:29.126167 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:29.134988 140200235099904 saver.py:1270] Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:29.496692 140200235099904 session_manager.py:491] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:30.901467 140200235099904 session_manager.py:493] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-05-08-13:55:47\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:47.821115 140200235099904 evaluation.py:277] Finished evaluation at 2019-05-08-13:55:47\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.02687605, global_step = 1000, loss = 5.4521894, map_at_k = 0.059553174603174636, top_k_accuracy = 0.16360015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:47.839330 140200235099904 estimator.py:1979] Saving dict for global step 1000: accuracy = 0.02687605, global_step = 1000, loss = 5.4521894, map_at_k = 0.059553174603174636, top_k_accuracy = 0.16360015\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: hybrid_recommendation_trained/model.ckpt-1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:48.069063 140200235099904 estimator.py:2039] Saving 'checkpoint_path' summary for global step 1000: hybrid_recommendation_trained/model.ckpt-1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:48.143230 140200235099904 estimator.py:1111] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:48.501814 140200235099904 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:48.585599 140200235099904 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:48.997986 140200235099904 estimator.py:1113] Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0508 13:55:49.007808 140200235099904 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:49.016834 140200235099904 export.py:587] Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:49.020572 140200235099904 export.py:587] Signatures INCLUDED in export for Regress: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:49.023398 140200235099904 export.py:587] Signatures INCLUDED in export for Train: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:49.026103 140200235099904 export.py:587] Signatures INCLUDED in export for Classify: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:49.027393 140200235099904 export.py:587] Signatures INCLUDED in export for Eval: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:49.111948 140200235099904 saver.py:1270] Restoring parameters from hybrid_recommendation_trained/model.ckpt-1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:49.419217 140200235099904 builder_impl.py:654] Assets added to graph.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: hybrid_recommendation_trained/export/exporter/temp-b'1557323748'/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:49.510589 140200235099904 builder_impl.py:763] Assets written to: hybrid_recommendation_trained/export/exporter/temp-b'1557323748'/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: hybrid_recommendation_trained/export/exporter/temp-b'1557323748'/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:50.038672 140200235099904 builder_impl.py:414] SavedModel written to: hybrid_recommendation_trained/export/exporter/temp-b'1557323748'/saved_model.pb\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 4.702016.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0508 13:55:50.827052 140200235099904 estimator.py:359] Loss for final step: 4.702016.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAkR--fKxn93",
        "colab_type": "text"
      },
      "source": [
        "## Run on module locally\n",
        "\n",
        "Now let's place our code into a python module with model.py and task.py files so that we can train using Google Cloud's ML Engine! First, let's test our module locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wCJUkRqxn93",
        "colab_type": "code",
        "colab": {},
        "outputId": "8a084e51-a109-4740-e540-f4154be45a95"
      },
      "source": [
        "%writefile requirements.txt\n",
        "tensorflow_hub"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7zFPGQSxn98",
        "colab_type": "code",
        "colab": {},
        "outputId": "6a335742-fcb1-48bc-9d32-c11cb4d90678"
      },
      "source": [
        "%%bash\n",
        "echo \"bucket=${BUCKET}\"\n",
        "rm -rf hybrid_recommendation_trained\n",
        "export PYTHONPATH=${PYTHONPATH}:${PWD}/hybrid_recommendations_module\n",
        "python -m trainer.task \\\n",
        "    --bucket=${BUCKET} \\\n",
        "    --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
        "    --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
        "    --output_dir=${OUTDIR} \\\n",
        "    --batch_size=128 \\\n",
        "    --learning_rate=0.1 \\\n",
        "    --hidden_units=\"256 128 64\" \\\n",
        "    --content_id_embedding_dimensions=10 \\\n",
        "    --author_embedding_dimensions=10 \\\n",
        "    --top_k=10 \\\n",
        "    --train_steps=1000 \\\n",
        "    --start_delay_secs=30 \\\n",
        "    --throttle_secs=60"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bucket=qwiklabs-gcp-d54a1ed2fe64d873\n",
            "number_of_content_ids = 15634\n",
            "number_of_categories = 3\n",
            "number_of_authors = 1103\n",
            "mean_months_since_epoch = 573.60733908\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0508 13:55:58.750374 140240352323328 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
            "INFO:tensorflow:vocabulary_size = 3 in category is inferred from the number of elements in the vocabulary_file gs://qwiklabs-gcp-d54a1ed2fe64d873/hybrid_recommendation/preproc/vocabs/category_vocab.txt-00000-of-00001.\n",
            "I0508 13:56:00.326117 140240352323328 feature_column_v2.py:1625] vocabulary_size = 3 in category is inferred from the number of elements in the vocabulary_file gs://qwiklabs-gcp-d54a1ed2fe64d873/hybrid_recommendation/preproc/vocabs/category_vocab.txt-00000-of-00001.\n",
            "INFO:tensorflow:Using default config.\n",
            "I0508 13:56:00.330513 140240352323328 estimator.py:1739] Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmppy222di0\n",
            "W0508 13:56:00.331419 140240352323328 estimator.py:1760] Using temporary folder as model directory: /tmp/tmppy222di0\n",
            "INFO:tensorflow:Using config: {'_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_device_fn': None, '_eval_distribute': None, '_service': None, '_save_summary_steps': 100, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_experimental_distribute': None, '_keep_checkpoint_max': 5, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8c1d90b710>, '_protocol': None, '_task_type': 'worker', '_num_worker_replicas': 1, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/tmppy222di0', '_train_distribute': None, '_task_id': 0, '_tf_random_seed': None, '_num_ps_replicas': 0, '_global_id_in_cluster': 0, '_evaluation_master': '', '_log_step_count_steps': 100, '_master': ''}\n",
            "I0508 13:56:00.332450 140240352323328 estimator.py:201] Using config: {'_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_device_fn': None, '_eval_distribute': None, '_service': None, '_save_summary_steps': 100, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_experimental_distribute': None, '_keep_checkpoint_max': 5, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8c1d90b710>, '_protocol': None, '_task_type': 'worker', '_num_worker_replicas': 1, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/tmppy222di0', '_train_distribute': None, '_task_id': 0, '_tf_random_seed': None, '_num_ps_replicas': 0, '_global_id_in_cluster': 0, '_evaluation_master': '', '_log_step_count_steps': 100, '_master': ''}\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "I0508 13:56:00.334890 140240352323328 estimator_training.py:185] Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "I0508 13:56:00.335288 140240352323328 training.py:610] Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "I0508 13:56:00.335692 140240352323328 training.py:698] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "W0508 13:56:00.355571 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0508 13:56:00.513999 140240352323328 estimator.py:1111] Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: EmbeddingColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.515632 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: EmbeddingColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3100: HashedCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.516270 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3100: HashedCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: HashedCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.516508 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: HashedCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3040: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.525848 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3040: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:206: EmbeddingColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.576765 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:206: EmbeddingColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: IndicatorColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.586103 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: IndicatorColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.586677 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: CrossedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.586873 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4295: CrossedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: CrossedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.587060 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: CrossedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4115: VocabularyFileCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.587254 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4115: VocabularyFileCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: VocabularyFileCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.587470 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: VocabularyFileCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4115: BucketizedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.600912 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4115: BucketizedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: BucketizedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.601409 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: BucketizedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.601629 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "W0508 13:56:00.603410 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2898: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "W0508 13:56:00.624511 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2898: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.632089 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.632550 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: VocabularyFileCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.647378 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:4321: VocabularyFileCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.723605 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0508 13:56:00.725599 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "2019-05-08 13:56:00.810602: W tensorflow/core/graph/graph_constructor.cc:1272] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0508 13:56:00.823500 140240352323328 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0508 13:56:00.871247 140240352323328 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
            "WARNING:tensorflow:From /content/datalab/training-data-analyst/courses/machine_learning/deepdive/10_recommend/hybrid_recommendations/hybrid_recommendations_module/trainer/model.py:161: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0508 13:56:00.958317 140240352323328 deprecation.py:323] From /content/datalab/training-data-analyst/courses/machine_learning/deepdive/10_recommend/hybrid_recommendations/hybrid_recommendations_module/trainer/model.py:161: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:2295: to_double (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "W0508 13:56:06.152451 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:2295: to_double (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:3040: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0508 13:56:06.159217 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/metrics_impl.py:3040: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "W0508 13:56:06.365407 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0508 13:56:06.466513 140240352323328 estimator.py:1113] Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0508 13:56:06.469039 140240352323328 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0508 13:56:06.866001 140240352323328 monitored_session.py:222] Graph was finalized.\n",
            "2019-05-08 13:56:06.866747: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2019-05-08 13:56:06.872927: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-08 13:56:06.873229: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x557bdd6d3220 executing computations on platform Host. Devices:\n",
            "2019-05-08 13:56:06.873248: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-08 13:56:06.997516: W tensorflow/core/framework/allocator.cc:124] Allocation of 195793000 exceeds 10% of system memory.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0508 13:56:07.266408 140240352323328 session_manager.py:491] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0508 13:56:08.585821 140240352323328 session_manager.py:493] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmppy222di0/model.ckpt.\n",
            "I0508 13:56:09.517179 140240352323328 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /tmp/tmppy222di0/model.ckpt.\n",
            "INFO:tensorflow:loss = 9.656839, step = 1\n",
            "I0508 13:56:10.833714 140240352323328 basic_session_run_hooks.py:249] loss = 9.656839, step = 1\n",
            "INFO:tensorflow:global_step/sec: 8.4283\n",
            "I0508 13:56:22.698155 140240352323328 basic_session_run_hooks.py:680] global_step/sec: 8.4283\n",
            "INFO:tensorflow:loss = 5.1725507, step = 101 (11.866 sec)\n",
            "I0508 13:56:22.699358 140240352323328 basic_session_run_hooks.py:247] loss = 5.1725507, step = 101 (11.866 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.08003\n",
            "I0508 13:56:33.711337 140240352323328 basic_session_run_hooks.py:680] global_step/sec: 9.08003\n",
            "INFO:tensorflow:loss = 4.4465528, step = 201 (11.013 sec)\n",
            "I0508 13:56:33.712445 140240352323328 basic_session_run_hooks.py:247] loss = 4.4465528, step = 201 (11.013 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.04784\n",
            "I0508 13:56:44.763681 140240352323328 basic_session_run_hooks.py:680] global_step/sec: 9.04784\n",
            "INFO:tensorflow:loss = 4.671628, step = 301 (11.052 sec)\n",
            "I0508 13:56:44.764770 140240352323328 basic_session_run_hooks.py:247] loss = 4.671628, step = 301 (11.052 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.2176\n",
            "I0508 13:56:55.612523 140240352323328 basic_session_run_hooks.py:680] global_step/sec: 9.2176\n",
            "INFO:tensorflow:loss = 4.6667438, step = 401 (10.849 sec)\n",
            "I0508 13:56:55.613710 140240352323328 basic_session_run_hooks.py:247] loss = 4.6667438, step = 401 (10.849 sec)\n",
            "INFO:tensorflow:global_step/sec: 8.90418\n",
            "I0508 13:57:06.843210 140240352323328 basic_session_run_hooks.py:680] global_step/sec: 8.90418\n",
            "INFO:tensorflow:loss = 4.2166204, step = 501 (11.231 sec)\n",
            "I0508 13:57:06.844383 140240352323328 basic_session_run_hooks.py:247] loss = 4.2166204, step = 501 (11.231 sec)\n",
            "INFO:tensorflow:global_step/sec: 8.98287\n",
            "I0508 13:57:17.975517 140240352323328 basic_session_run_hooks.py:680] global_step/sec: 8.98287\n",
            "INFO:tensorflow:loss = 4.4779973, step = 601 (11.132 sec)\n",
            "I0508 13:57:17.976780 140240352323328 basic_session_run_hooks.py:247] loss = 4.4779973, step = 601 (11.132 sec)\n",
            "INFO:tensorflow:global_step/sec: 4.35725\n",
            "I0508 13:57:40.925844 140240352323328 basic_session_run_hooks.py:680] global_step/sec: 4.35725\n",
            "INFO:tensorflow:loss = 4.6467867, step = 701 (22.950 sec)\n",
            "I0508 13:57:40.927129 140240352323328 basic_session_run_hooks.py:247] loss = 4.6467867, step = 701 (22.950 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.70045\n",
            "I0508 13:58:17.956748 140240352323328 basic_session_run_hooks.py:680] global_step/sec: 2.70045\n",
            "INFO:tensorflow:loss = 4.3240857, step = 801 (37.041 sec)\n",
            "I0508 13:58:17.968600 140240352323328 basic_session_run_hooks.py:247] loss = 4.3240857, step = 801 (37.041 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.59355\n",
            "I0508 13:58:56.513956 140240352323328 basic_session_run_hooks.py:680] global_step/sec: 2.59355\n",
            "INFO:tensorflow:loss = 5.1733055, step = 901 (38.547 sec)\n",
            "I0508 13:58:56.515249 140240352323328 basic_session_run_hooks.py:247] loss = 5.1733055, step = 901 (38.547 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmppy222di0/model.ckpt.\n",
            "I0508 13:59:26.506852 140240352323328 basic_session_run_hooks.py:594] Saving checkpoints for 1000 into /tmp/tmppy222di0/model.ckpt.\n",
            "2019-05-08 13:59:26.552259: W tensorflow/core/framework/allocator.cc:124] Allocation of 195793000 exceeds 10% of system memory.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0508 13:59:27.705708 140240352323328 estimator.py:1111] Calling model_fn.\n",
            "2019-05-08 13:59:28.001518: W tensorflow/core/graph/graph_constructor.cc:1272] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0508 13:59:28.014291 140240352323328 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0508 13:59:28.113040 140240352323328 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0508 13:59:28.963687 140240352323328 estimator.py:1113] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-05-08T13:59:28Z\n",
            "I0508 13:59:28.989470 140240352323328 evaluation.py:257] Starting evaluation at 2019-05-08T13:59:28Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0508 13:59:29.123131 140240352323328 monitored_session.py:222] Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0508 13:59:29.124109 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmppy222di0/model.ckpt-1000\n",
            "I0508 13:59:29.125705 140240352323328 saver.py:1270] Restoring parameters from /tmp/tmppy222di0/model.ckpt-1000\n",
            "2019-05-08 13:59:29.194589: W tensorflow/core/framework/allocator.cc:124] Allocation of 195793000 exceeds 10% of system memory.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0508 13:59:29.461488 140240352323328 session_manager.py:491] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0508 13:59:30.758937 140240352323328 session_manager.py:493] Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-05-08-13:59:47\n",
            "I0508 13:59:47.058863 140240352323328 evaluation.py:277] Finished evaluation at 2019-05-08-13:59:47\n",
            "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.028751124, global_step = 1000, loss = 5.457616, map_at_k = 0.052008134920634934, top_k_accuracy = 0.19465604\n",
            "I0508 13:59:47.059537 140240352323328 estimator.py:1979] Saving dict for global step 1000: accuracy = 0.028751124, global_step = 1000, loss = 5.457616, map_at_k = 0.052008134920634934, top_k_accuracy = 0.19465604\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmppy222di0/model.ckpt-1000\n",
            "I0508 13:59:47.273035 140240352323328 estimator.py:2039] Saving 'checkpoint_path' summary for global step 1000: /tmp/tmppy222di0/model.ckpt-1000\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0508 13:59:47.370150 140240352323328 estimator.py:1111] Calling model_fn.\n",
            "2019-05-08 13:59:47.672790: W tensorflow/core/graph/graph_constructor.cc:1272] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0508 13:59:47.686474 140240352323328 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "I0508 13:59:47.741057 140240352323328 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0508 13:59:48.134108 140240352323328 estimator.py:1113] Done calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0508 13:59:48.135222 140240352323328 deprecation.py:323] From /usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "I0508 13:59:48.137349 140240352323328 export.py:587] Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "I0508 13:59:48.137587 140240352323328 export.py:587] Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "I0508 13:59:48.137754 140240352323328 export.py:587] Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "I0508 13:59:48.137849 140240352323328 export.py:587] Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
            "I0508 13:59:48.137970 140240352323328 export.py:587] Signatures INCLUDED in export for Predict: ['predict_export_outputs', 'serving_default']\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmppy222di0/model.ckpt-1000\n",
            "I0508 13:59:48.452034 140240352323328 saver.py:1270] Restoring parameters from /tmp/tmppy222di0/model.ckpt-1000\n",
            "2019-05-08 13:59:48.520767: W tensorflow/core/framework/allocator.cc:124] Allocation of 195793000 exceeds 10% of system memory.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "I0508 13:59:48.727722 140240352323328 builder_impl.py:654] Assets added to graph.\n",
            "INFO:tensorflow:Assets written to: /tmp/tmppy222di0/export/exporter/temp-b'1557323987'/assets\n",
            "I0508 13:59:48.796345 140240352323328 builder_impl.py:763] Assets written to: /tmp/tmppy222di0/export/exporter/temp-b'1557323987'/assets\n",
            "INFO:tensorflow:SavedModel written to: /tmp/tmppy222di0/export/exporter/temp-b'1557323987'/saved_model.pb\n",
            "I0508 13:59:49.308889 140240352323328 builder_impl.py:414] SavedModel written to: /tmp/tmppy222di0/export/exporter/temp-b'1557323987'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 4.5024443.\n",
            "I0508 13:59:50.006966 140240352323328 estimator.py:359] Loss for final step: 4.5024443.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NlSb5Qoxn9_",
        "colab_type": "text"
      },
      "source": [
        "# Run on Google Cloud ML Engine\n",
        "If our module locally trained fine, let's now use of the power of ML Engine to scale it out on Google Cloud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL-xfvSrxn9_",
        "colab_type": "code",
        "colab": {},
        "outputId": "3074c2d6-2e48-47b3-9b82-64956cb41b58"
      },
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/hybrid_recommendation/small_trained_model\n",
        "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "    --region=$REGION \\\n",
        "    --module-name=trainer.task \\\n",
        "    --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
        "    --job-dir=$OUTDIR \\\n",
        "    --staging-bucket=gs://$BUCKET \\\n",
        "    --scale-tier=STANDARD_1 \\\n",
        "    --runtime-version=$TFVERSION \\\n",
        "    -- \\\n",
        "    --bucket=${BUCKET} \\\n",
        "    --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
        "    --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
        "    --output_dir=${OUTDIR} \\\n",
        "    --batch_size=128 \\\n",
        "    --learning_rate=0.1 \\\n",
        "    --hidden_units=\"256 128 64\" \\\n",
        "    --content_id_embedding_dimensions=10 \\\n",
        "    --author_embedding_dimensions=10 \\\n",
        "    --top_k=10 \\\n",
        "    --train_steps=1000 \\\n",
        "    --start_delay_secs=30 \\\n",
        "    --throttle_secs=30"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://qwiklabs-gcp-d54a1ed2fe64d873/hybrid_recommendation/small_trained_model us-central1 hybrid_recommendation_190508_135951\n",
            "jobId: hybrid_recommendation_190508_135951\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "CommandException: 1 files/objects could not be removed.\n",
            "Job [hybrid_recommendation_190508_135951] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ml-engine jobs describe hybrid_recommendation_190508_135951\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ml-engine jobs stream-logs hybrid_recommendation_190508_135951\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck6evQQyxn-C",
        "colab_type": "text"
      },
      "source": [
        "Let's add some hyperparameter tuning!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fbs9tTAxn-D",
        "colab_type": "code",
        "colab": {},
        "outputId": "43366dfd-29e2-4496-fc9a-1aaff651a436"
      },
      "source": [
        "%%writefile hyperparam.yaml\n",
        "trainingInput:\n",
        "    hyperparameters:\n",
        "        goal: MAXIMIZE\n",
        "        maxTrials: 5\n",
        "        maxParallelTrials: 1\n",
        "        hyperparameterMetricTag: accuracy\n",
        "        params:\n",
        "            - parameterName: batch_size\n",
        "              type: INTEGER\n",
        "              minValue: 8\n",
        "              maxValue: 64\n",
        "              scaleType: UNIT_LINEAR_SCALE\n",
        "            - parameterName: learning_rate\n",
        "              type: DOUBLE\n",
        "              minValue: 0.01\n",
        "              maxValue: 0.1\n",
        "              scaleType: UNIT_LINEAR_SCALE\n",
        "            - parameterName: hidden_units\n",
        "              type: CATEGORICAL\n",
        "              categoricalValues: [\"1024 512 256\", \"1024 512 128\", \"1024 256 128\", \"512 256 128\", \"1024 512 64\", \"1024 256 64\", \"512 256 64\", \"1024 128 64\", \"512 128 64\", \"256 128 64\", \"1024 512 32\", \"1024 256 32\", \"512 256 32\", \"1024 128 32\", \"512 128 32\", \"256 128 32\", \"1024 64 32\", \"512 64 32\", \"256 64 32\", \"128 64 32\"]\n",
        "            - parameterName: content_id_embedding_dimensions\n",
        "              type: INTEGER\n",
        "              minValue: 5\n",
        "              maxValue: 250\n",
        "              scaleType: UNIT_LOG_SCALE\n",
        "            - parameterName: author_embedding_dimensions\n",
        "              type: INTEGER\n",
        "              minValue: 5\n",
        "              maxValue: 30\n",
        "              scaleType: UNIT_LINEAR_SCALE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing hyperparam.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0iaXOzbxn-H",
        "colab_type": "code",
        "colab": {},
        "outputId": "50f6947d-f9e2-4489-b19d-9b489fdd1538"
      },
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/hybrid_recommendation/hypertuning\n",
        "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "    --region=$REGION \\\n",
        "    --module-name=trainer.task \\\n",
        "    --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
        "    --job-dir=$OUTDIR \\\n",
        "    --staging-bucket=gs://$BUCKET \\\n",
        "    --scale-tier=STANDARD_1 \\\n",
        "    --runtime-version=$TFVERSION \\\n",
        "    --config=hyperparam.yaml \\\n",
        "    -- \\\n",
        "    --bucket=${BUCKET} \\\n",
        "    --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
        "    --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
        "    --output_dir=${OUTDIR} \\\n",
        "    --batch_size=128 \\\n",
        "    --learning_rate=0.1 \\\n",
        "    --hidden_units=\"256 128 64\" \\\n",
        "    --content_id_embedding_dimensions=10 \\\n",
        "    --author_embedding_dimensions=10 \\\n",
        "    --top_k=10 \\\n",
        "    --train_steps=1000 \\\n",
        "    --start_delay_secs=30 \\\n",
        "    --throttle_secs=30"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://qwiklabs-gcp-d54a1ed2fe64d873/hybrid_recommendation/hypertuning us-central1 hybrid_recommendation_190508_140000\n",
            "jobId: hybrid_recommendation_190508_140000\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "CommandException: 1 files/objects could not be removed.\n",
            "Job [hybrid_recommendation_190508_140000] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ml-engine jobs describe hybrid_recommendation_190508_140000\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ml-engine jobs stream-logs hybrid_recommendation_190508_140000\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuXO8tGrxn-K",
        "colab_type": "text"
      },
      "source": [
        "Now that we know the best hyperparameters, run a big training job!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LejwXZlyxn-L",
        "colab_type": "code",
        "colab": {},
        "outputId": "e35c5ae1-9db1-4cb5-eddc-a0bafcbd7204"
      },
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/hybrid_recommendation/big_trained_model\n",
        "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "    --region=$REGION \\\n",
        "    --module-name=trainer.task \\\n",
        "    --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
        "    --job-dir=$OUTDIR \\\n",
        "    --staging-bucket=gs://$BUCKET \\\n",
        "    --scale-tier=STANDARD_1 \\\n",
        "    --runtime-version=$TFVERSION \\\n",
        "    -- \\\n",
        "    --bucket=${BUCKET} \\\n",
        "    --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
        "    --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
        "    --output_dir=${OUTDIR} \\\n",
        "    --batch_size=128 \\\n",
        "    --learning_rate=0.1 \\\n",
        "    --hidden_units=\"256 128 64\" \\\n",
        "    --content_id_embedding_dimensions=10 \\\n",
        "    --author_embedding_dimensions=10 \\\n",
        "    --top_k=10 \\\n",
        "    --train_steps=10000 \\\n",
        "    --start_delay_secs=30 \\\n",
        "    --throttle_secs=30"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://qwiklabs-gcp-d54a1ed2fe64d873/hybrid_recommendation/big_trained_model us-central1 hybrid_recommendation_190508_140004\n",
            "jobId: hybrid_recommendation_190508_140004\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "CommandException: 1 files/objects could not be removed.\n",
            "Job [hybrid_recommendation_190508_140004] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ml-engine jobs describe hybrid_recommendation_190508_140004\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ml-engine jobs stream-logs hybrid_recommendation_190508_140004\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}