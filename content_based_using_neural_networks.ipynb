{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "content_based_using_neural_networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkrissada/google_ml_training/blob/master/content_based_using_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYRC7PGqqpsw",
        "colab_type": "text"
      },
      "source": [
        "## Content-Based Filtering Using Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tXdoJ_Dqpsz",
        "colab_type": "text"
      },
      "source": [
        "This lab relies on files created in the [content_based_preproc.ipynb](./content_based_preproc.ipynb) notebook. Be sure to complete the TODOs in that notebook and run the code there before completing this lab.  \n",
        "Also, we'll be using the **python3** kernel from here on out so don't forget to change the kernel if it's still python2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1vkr0c0qpsz",
        "colab_type": "text"
      },
      "source": [
        "This lab illustrates:\n",
        "1. how to build feature columns for a model using tf.feature_column\n",
        "2. how to create custom evaluation metrics and add them to Tensorboard\n",
        "3. how to train a model and make predictions with the saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9uYiDr8qps0",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow Hub should already be installed. You can check using pip freeze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDcqc-xgqps1",
        "colab_type": "code",
        "colab": {},
        "outputId": "d27c714b-58db-4b91-bce5-18aa8ed42a91"
      },
      "source": [
        "%%bash\n",
        "pip freeze | grep tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow==1.8.0\n",
            "tensorflow-hub==0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOvsN3Bjqps8",
        "colab_type": "text"
      },
      "source": [
        "If 'tensorflow-hub' isn't one of the outputs above, then you'll need to install it. Uncomment the cell below and execute the commands. After doing the pip install, click **\"Reset Session\"** on the notebook so that the Python environment picks up the new packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcfALYzmqps9",
        "colab_type": "code",
        "colab": {},
        "outputId": "b7c6e87b-98e0-4955-93f2-5420c1346986"
      },
      "source": [
        "!pip3 install tensorflow-hub==0.4.0\n",
        "!pip3 install --upgrade tensorflow==1.13.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub==0.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-hub==0.4.0) (3.6.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-hub==0.4.0) (1.14.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-hub==0.4.0) (1.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.4.0->tensorflow-hub==0.4.0) (40.2.0)\n",
            "Collecting tensorflow==1.13.1\n",
            "  Using cached https://files.pythonhosted.org/packages/ca/f2/0931c194bb98398017d52c94ee30e5e1a4082ab6af76e204856ff1fdb33e/tensorflow-1.13.1-cp35-cp35m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.17.1)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.31.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.0.7)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
            "  Using cached https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.0.9)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (3.6.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow==1.13.1) (1.14.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.13.1)\n",
            "  Using cached https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/envs/py3env/lib/python3.5/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.7.1)\n",
            "Requirement already satisfied, skipping upgrade: mock>=2.0.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (40.2.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.6.11)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: pbr>=0.11 in /usr/local/envs/py3env/lib/python3.5/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.2.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow 1.8.0\n",
            "    Uninstalling tensorflow-1.8.0:\n",
            "      Successfully uninstalled tensorflow-1.8.0\n",
            "Successfully installed tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcCNEU0QqptB",
        "colab_type": "code",
        "colab": {},
        "outputId": "de703ab9-4d27-4124-c16f-43827270ff02"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import shutil\n",
        "\n",
        "PROJECT = 'qwiklabs-gcp-1d5aaaec2a18ca9e' # REPLACE WITH YOUR PROJECT ID\n",
        "BUCKET = 'qwiklabs-gcp-1d5aaaec2a18ca9e' # REPLACE WITH YOUR BUCKET NAME\n",
        "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
        "\n",
        "# do not change these\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['REGION'] = REGION\n",
        "os.environ['TFVERSION'] = '1.13.1'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0507 14:42:38.573041 140047640102656 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKh_MhetqptF",
        "colab_type": "code",
        "colab": {},
        "outputId": "83359418-3ac8-40bd-e195-d9967ea4594e"
      },
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urgZQNDOqptI",
        "colab_type": "text"
      },
      "source": [
        "### Build the feature columns for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01aRcqDKqptJ",
        "colab_type": "text"
      },
      "source": [
        "To start, we'll load the list of categories, authors and article ids we created in the previous **Create Datasets** notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nw0H774qptK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#categories_list = open(\"categories.txt\").read().splitlines()\n",
        "authors_list = open(\"authors.txt\").read().splitlines()\n",
        "content_ids_list = open(\"content_ids.txt\").read().splitlines()\n",
        "mean_months_since_epoch = 523"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3YtNhacqptS",
        "colab_type": "code",
        "colab": {},
        "outputId": "9dd85865-7105-4804-81e9-05ee71ee40fb"
      },
      "source": [
        "%bash\n",
        "ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 30516\n",
            "-rw-r--r-- 1 root root     6114 May  7 14:33 authors.txt\n",
            "drwxr-xr-x 5 root root     4096 May  7 14:30 composer_gcf_trigger\n",
            "-rw-r--r-- 1 root root    12213 May  7 14:30 content_based_by_hand.ipynb\n",
            "-rw-r--r-- 1 root root    25978 May  7 14:37 content_based_preproc.ipynb\n",
            "-rw-r--r-- 1 root root    22817 May  7 14:42 content_based_using_neural_networks.ipynb\n",
            "-rw-r--r-- 1 root root   149675 May  7 14:33 content_ids.txt\n",
            "drwxr-xr-x 7 root root     4096 May  7 14:30 endtoend\n",
            "drwxr-xr-x 3 root root     4096 May  7 14:30 hybrid_recommendations\n",
            "-rw-r--r-- 1 root root  3076960 May  7 14:36 test_set.csv\n",
            "-rw-r--r-- 1 root root 27891159 May  7 14:35 training_set.csv\n",
            "-rw-r--r-- 1 root root    32433 May  7 14:30 wals.ipynb\n",
            "drwxr-xr-x 2 root root     4096 May  7 14:30 walsmodel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ewPlQkoqptV",
        "colab_type": "text"
      },
      "source": [
        "In the cell below we'll define the feature columns to use in our model. If necessary, remind yourself the [various feature columns](https://www.tensorflow.org/api_docs/python/tf/feature_column) to use.  \n",
        "For the embedded_title_column feature column, use a Tensorflow Hub Module to create an embedding of the article title. Since the articles and titles are in German, you'll want to use a German language embedding module.  \n",
        "Explore the text embedding Tensorflow Hub modules [available here](https://alpha.tfhub.dev/). Filter by setting the language to 'German'. The 50 dimensional embedding should be sufficient for our purposes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j58nuO-mqptW",
        "colab_type": "code",
        "colab": {},
        "outputId": "07cd7ed4-d6fd-42f7-ab0a-de137b4e2aa8"
      },
      "source": [
        "embedded_title_column = hub.text_embedding_column(\n",
        "    key=\"title\", \n",
        "    module_spec=\"https://tfhub.dev/google/nnlm-de-dim50/1\",\n",
        "    trainable=False)\n",
        "\n",
        "content_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "    key=\"content_id\",\n",
        "    hash_bucket_size= len(content_ids_list) + 1)\n",
        "embedded_content_column = tf.feature_column.embedding_column(\n",
        "    categorical_column=content_id_column,\n",
        "    dimension=10)\n",
        "\n",
        "author_column = tf.feature_column.categorical_column_with_hash_bucket(key=\"author\",\n",
        "    hash_bucket_size=len(authors_list) + 1)\n",
        "embedded_author_column = tf.feature_column.embedding_column(\n",
        "    categorical_column=author_column,\n",
        "    dimension=3)\n",
        "\n",
        "category_column_categorical = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "    key=\"category\",\n",
        "    vocabulary_list=categories_list,\n",
        "    num_oov_buckets=1)\n",
        "category_column = tf.feature_column.indicator_column(category_column_categorical)\n",
        "\n",
        "months_since_epoch_boundaries = list(range(400,700,20))\n",
        "months_since_epoch_column = tf.feature_column.numeric_column(\n",
        "    key=\"months_since_epoch\")\n",
        "months_since_epoch_bucketized = tf.feature_column.bucketized_column(\n",
        "    source_column = months_since_epoch_column,\n",
        "    boundaries = months_since_epoch_boundaries)\n",
        "\n",
        "crossed_months_since_category_column = tf.feature_column.indicator_column(tf.feature_column.crossed_column(\n",
        "  keys = [category_column_categorical, months_since_epoch_bucketized], \n",
        "  hash_bucket_size = len(months_since_epoch_boundaries) * (len(categories_list) + 1)))\n",
        "\n",
        "feature_columns = [embedded_content_column,\n",
        "                   embedded_author_column,\n",
        "                   category_column,\n",
        "                   embedded_title_column,\n",
        "                   crossed_months_since_category_column] "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'categories_list' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1a848671636c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m category_column_categorical = tf.feature_column.categorical_column_with_vocabulary_list(\n\u001b[1;32m     20\u001b[0m     \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mvocabulary_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategories_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     num_oov_buckets=1)\n\u001b[1;32m     23\u001b[0m \u001b[0mcategory_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicator_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_column_categorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'categories_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hiDTBt6qptZ",
        "colab_type": "text"
      },
      "source": [
        "### Create the input function.\n",
        "\n",
        "Next we'll create the input function for our model. This input function reads the data from the csv files we created in the previous labs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pw9nM3Jqpta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "record_defaults = [[\"Unknown\"], [\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[\"Unknown\"],[mean_months_since_epoch],[\"Unknown\"]]\n",
        "column_keys = [\"visitor_id\", \"content_id\", \"category\", \"title\", \"author\", \"months_since_epoch\", \"next_content_id\"]\n",
        "label_key = \"next_content_id\"\n",
        "def read_dataset(filename, mode, batch_size = 512):\n",
        "  def _input_fn():\n",
        "      def decode_csv(value_column):\n",
        "          columns = tf.decode_csv(value_column,record_defaults=record_defaults)\n",
        "          features = dict(zip(column_keys, columns))          \n",
        "          label = features.pop(label_key)         \n",
        "          return features, label\n",
        "\n",
        "      # Create list of files that match pattern\n",
        "      file_list = tf.gfile.Glob(filename)\n",
        "\n",
        "      # Create dataset from file list\n",
        "      dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "          num_epochs = None # indefinitely\n",
        "          dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "      else:\n",
        "          num_epochs = 1 # end-of-input after this\n",
        "\n",
        "      dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "      return dataset.make_one_shot_iterator().get_next()\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au7Iubv5qpth",
        "colab_type": "text"
      },
      "source": [
        "### Create the model and train/evaluate\n",
        "\n",
        "\n",
        "Next, we'll build our model which recommends an article for a visitor to the Kurier.at website. Look through the code below. We use the input_layer feature column to create the dense input layer to our network. This is just a sigle layer network where we can adjust the number of hidden units as a parameter.\n",
        "\n",
        "Currently, we compute the accuracy between our predicted 'next article' and the actual 'next article' read next by the visitor. Resolve the TODOs in the cell below by adding additional performance metrics to assess our model. You will need to \n",
        "* use the [tf.metrics library](https://www.tensorflow.org/api_docs/python/tf/metrics) to compute an additional performance metric\n",
        "* add this additional metric to the metrics dictionary, and \n",
        "* include it in the tf.summary that is sent to Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSj2gKw1qpti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "  net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
        "  for units in params['hidden_units']:\n",
        "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
        "   # Compute logits (1 per class).\n",
        "  logits = tf.layers.dense(net, params['n_classes'], activation=None) \n",
        "\n",
        "  predicted_classes = tf.argmax(logits, 1)\n",
        "  from tensorflow.python.lib.io import file_io\n",
        "    \n",
        "  with file_io.FileIO('content_ids.txt', mode='r') as ifp:\n",
        "    content = tf.constant([x.rstrip() for x in ifp])\n",
        "  predicted_class_names = tf.gather(content, predicted_classes)\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {\n",
        "        'class_ids': predicted_classes[:, tf.newaxis],\n",
        "        'class_names' : predicted_class_names[:, tf.newaxis],\n",
        "        'probabilities': tf.nn.softmax(logits),\n",
        "        'logits': logits,\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "  table = tf.contrib.lookup.index_table_from_file(vocabulary_file=\"content_ids.txt\")\n",
        "  labels = table.lookup(labels)\n",
        "  # Compute loss.\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "  # Compute evaluation metrics.\n",
        "  accuracy = tf.metrics.accuracy(labels=labels,\n",
        "                                 predictions=predicted_classes,\n",
        "                                 name='acc_op')\n",
        "  \n",
        "  top_10_accuracy = tf.metrics.mean(tf.nn.in_top_k(predictions=logits, \n",
        "                                                   targets=labels, \n",
        "                                                   k=10))\n",
        "  \n",
        "  metrics = {\n",
        "    'accuracy': accuracy,\n",
        "    'top_10_accuracy' : top_10_accuracy}\n",
        "  \n",
        "  tf.summary.scalar('accuracy', accuracy[1])\n",
        "  tf.summary.scalar('top_10_accuracy', top_10_accuracy[1])\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "      return tf.estimator.EstimatorSpec(\n",
        "          mode, loss=loss, eval_metric_ops=metrics)\n",
        "\n",
        "  # Create training op.\n",
        "  assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "  optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
        "  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxlgThfOqptl",
        "colab_type": "text"
      },
      "source": [
        "### Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUReigY2qptl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outdir = 'content_based_model_trained'\n",
        "shutil.rmtree(outdir, ignore_errors = True) # start fresh each time\n",
        "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    model_dir = outdir,\n",
        "    params={\n",
        "     'feature_columns': feature_columns,\n",
        "      'hidden_units': [200, 100, 50],\n",
        "      'n_classes': len(content_ids_list)\n",
        "    })\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "    input_fn = read_dataset(\"training_set.csv\", tf.estimator.ModeKeys.TRAIN),\n",
        "    max_steps = 200)\n",
        "\n",
        "eval_spec = tf.estimator.EvalSpec(\n",
        "    input_fn = read_dataset(\"test_set.csv\", tf.estimator.ModeKeys.EVAL),\n",
        "    steps = None,\n",
        "    start_delay_secs = 30,\n",
        "    throttle_secs = 60)\n",
        "\n",
        "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIqJA74tqptn",
        "colab_type": "text"
      },
      "source": [
        "### Make predictions with the trained model. \n",
        "\n",
        "With the model now trained, we can make predictions by calling the predict method on the estimator. Let's look at how our model predicts on the first five examples of the training set.  \n",
        "To start, we'll create a new file 'first_5.csv' which contains the first five elements of our training set. We'll also save the target values to a file 'first_5_content_ids' so we can compare our results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUjNUFvdqpto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "head -5 training_set.csv > first_5.csv\n",
        "head first_5.csv\n",
        "awk -F \"\\\"*,\\\"*\" '{print $2}' first_5.csv > first_5_content_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NqXyInmqptq",
        "colab_type": "text"
      },
      "source": [
        "Recall, to make predictions on the trained model we pass a list of examples through the input function. Complete the code below to make predicitons on the examples contained in the \"first_5.csv\" file we created above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uidfh2ouqpts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = list(estimator.predict(input_fn=read_dataset(\"first_5.csv\", tf.estimator.ModeKeys.PREDICT)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePwAWh__qptv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "recommended_content_ids = [np.asscalar(d[\"class_names\"]).decode('UTF-8') for d in output]\n",
        "content_ids = open(\"first_5_content_ids\").read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybEyaqGsqptz",
        "colab_type": "text"
      },
      "source": [
        "Finally, we'll map the content id back to the article title. We can then compare our model's recommendation for the first of our examples. This can all be done in BigQuery. Look through the query below and make sure it is clear what is being returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHRio61bqpt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import google.datalab.bigquery as bq\n",
        "recommended_title_sql=\"\"\"\n",
        "#standardSQL\n",
        "SELECT\n",
        "(SELECT MAX(IF(index=6, value, NULL)) FROM UNNEST(hits.customDimensions)) AS title\n",
        "FROM `cloud-training-demos.GA360_test.ga_sessions_sample`,   \n",
        "  UNNEST(hits) AS hits\n",
        "WHERE \n",
        "  # only include hits on pages\n",
        "  hits.type = \"PAGE\"\n",
        "  AND (SELECT MAX(IF(index=10, value, NULL)) FROM UNNEST(hits.customDimensions)) = \\\"{}\\\"\n",
        "LIMIT 1\"\"\".format(recommended_content_ids[0])\n",
        "\n",
        "current_title_sql=\"\"\"\n",
        "#standardSQL\n",
        "SELECT\n",
        "(SELECT MAX(IF(index=6, value, NULL)) FROM UNNEST(hits.customDimensions)) AS title\n",
        "FROM `cloud-training-demos.GA360_test.ga_sessions_sample`,   \n",
        "  UNNEST(hits) AS hits\n",
        "WHERE \n",
        "  # only include hits on pages\n",
        "  hits.type = \"PAGE\"\n",
        "  AND (SELECT MAX(IF(index=10, value, NULL)) FROM UNNEST(hits.customDimensions)) = \\\"{}\\\"\n",
        "LIMIT 1\"\"\".format(content_ids[0])\n",
        "recommended_title = bq.Query(recommended_title_sql).execute().result().to_dataframe()['title'].tolist()[0]\n",
        "current_title = bq.Query(current_title_sql).execute().result().to_dataframe()['title'].tolist()[0]\n",
        "print(\"Current title: {} \".format(current_title))\n",
        "print(\"Recommended title: {}\".format(recommended_title))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGKDo6uzqpt3",
        "colab_type": "text"
      },
      "source": [
        "### Tensorboard\n",
        "\n",
        "As usual, we can monitor the performance of our training job using Tensorboard. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gOXbGT6qpt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.datalab.ml import TensorBoard\n",
        "TensorBoard().start('content_based_model_trained')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S8pmt-YqpuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for pid in TensorBoard.list()['pid']:\n",
        "  TensorBoard().stop(pid)\n",
        "  print(\"Stopped TensorBoard with pid {}\".format(pid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KeP1XdoqpuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}