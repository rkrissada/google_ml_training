{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tftransform.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkrissada/google_ml_training/blob/master/tftransform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6VHkQQCVvB6H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Exploring tf.transform </h1>\n",
        "\n",
        "While Pandas is fine for experimenting, for operationalization of your workflow, it is better to do preprocessing in Apache Beam. This will also help if you need to preprocess data in flight, since Apache Beam also allows for streaming.\n",
        "\n",
        "Only specific combinations of TensorFlow/Beam are supported by tf.transform. So make sure to get a combo that is.\n",
        "\n",
        "* TFT 0.8.0\n",
        "* TF 1.8 or higher\n",
        "* Apache Beam [GCP] 2.9.0 or higher"
      ]
    },
    {
      "metadata": {
        "id": "wfjjNkkavB6I",
        "colab_type": "code",
        "colab": {},
        "outputId": "5ad1b988-3b5b-46eb-fa1d-54a2f287357d"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install apache-beam[gcp]==2.9.0 tensorflow_transform==0.8.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting apache-beam[gcp]==2.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/3d/90aa15779e884feebae4b0c26cad6f52cd4040397a94deb58dad9c8b7300/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl (2.4MB)\n",
            "Collecting tensorflow_transform==0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/5c/d22108832b9df5d91e7750e544e69157c4eeca8321c177b83c047f428d9c/tensorflow-transform-0.8.0.tar.gz (122kB)\n",
            "Requirement already satisfied: dill<=0.2.8.2,>=0.2.6 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (0.2.6)\n",
            "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (2.0.0)\n",
            "Collecting httplib2<=0.11.3,>=0.8 (from apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/ce/aa4a385e3e9fd351737fd2b07edaa56e7a730448465aceda6b35086a0d9b/httplib2-0.11.3.tar.gz (215kB)\n",
            "Requirement already satisfied: pyyaml<4.0.0,>=3.12 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (3.13)\n",
            "Collecting typing<3.7.0,>=3.6.0; python_version < \"3.5.0\" (from apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/3e/29f92b7aeda5b078c86d14f550bf85cff809042e3429ace7af6193c3bc9f/typing-3.6.6-py2-none-any.whl\n",
            "Collecting pyvcf<0.7.0,>=0.6.8 (from apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/20/b6/36bfb1760f6983788d916096193fc14c83cce512c7787c93380e09458c09/PyVCF-0.6.8.tar.gz\n",
            "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (2.2.0)\n",
            "Requirement already satisfied: future<1.0.0,>=0.16.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (0.16.0)\n",
            "Requirement already satisfied: avro<2.0.0,>=1.8.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (1.8.2)\n",
            "Collecting pydot<1.3,>=1.2.0 (from apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/f1/e61d6dfe6c1768ed2529761a68f70939e2569da043e9f15a8d84bf56cadf/pydot-1.2.4.tar.gz (132kB)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (1.7)\n",
            "Collecting fastavro<0.22,>=0.21.4 (from apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/f2/d2e43dad688eb84db96c191905ee0a142a96ed969b33e90ddc861c2f4bb9/fastavro-0.21.22-cp27-cp27mu-manylinux1_x86_64.whl (1.0MB)\n",
            "Requirement already satisfied: futures<4.0.0,>=3.1.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (3.2.0)\n",
            "Requirement already satisfied: grpcio<2,>=1.8 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (1.17.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.5.0.post1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (3.6.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/38/72/59b384a9fa98ff6060ce2f186ea2e3d7a0491cc784d99d1637e0d1803df2/hdfs-2.5.2.tar.gz\n",
            "Requirement already satisfied: pytz<=2018.4,>=2018.3 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (2018.4)\n",
            "Collecting google-cloud-pubsub==0.35.4; extra == \"gcp\" (from apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/66/f9/bfa284399fb59a8896e0c4164b46185f61f35a90a18c67b366406ad472a6/google_cloud_pubsub-0.35.4-py2.py3-none-any.whl (93kB)\n",
            "Requirement already satisfied: proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (0.90.0)\n",
            "Collecting google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\" (from apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/26/abea123a1b5a2b0c1b49c0d8a2e030725f32ae0932d026f2c7a6ee32c8d3/google_apitools-0.5.24-py2-none-any.whl (129kB)\n",
            "Collecting google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" (from apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/1b/2b95f2fefddbbece38110712c225bfb5649206f4056445653bd5ca4dc86d/google_cloud_bigquery-1.6.1-py2.py3-none-any.whl (83kB)\n",
            "Requirement already satisfied: googledatastore<7.1,>=7.0.1; python_version < \"3.0\" and extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (7.0.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensorflow_transform==0.8.0) (0.6.1)\n",
            "Requirement already satisfied: numpy<2,>=1.13.3 in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensorflow_transform==0.8.0) (1.14.0)\n",
            "Requirement already satisfied: six<2,>=1.10 in /usr/local/envs/py2env/lib/python2.7/site-packages (from tensorflow_transform==0.8.0) (1.10.0)\n",
            "Requirement already satisfied: funcsigs>=1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.9.0) (1.0.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.9.0) (5.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/py2env/lib/python2.7/site-packages (from pyvcf<0.7.0,>=0.6.8->apache-beam[gcp]==2.9.0) (40.6.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.9.0) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.9.0) (0.2.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.9.0) (3.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from pydot<1.3,>=1.2.0->apache-beam[gcp]==2.9.0) (2.3.0)\n",
            "Requirement already satisfied: enum34>=1.0.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from grpcio<2,>=1.8->apache-beam[gcp]==2.9.0) (1.1.6)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (2.18.4)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=0.1.3 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (0.1.4)\n",
            "Collecting grpc-google-iam-v1<0.12dev,>=0.11.1 (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/28/f26f67381cb23e81271b8d66c00a846ad9d25a909ae1ae1df8222fad2744/grpc-google-iam-v1-0.11.4.tar.gz\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.5.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (1.5.5)\n",
            "Collecting fasteners>=0.14 (from google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/14/3a/096c7ad18e102d4f219f5dd15951f9728ca5092a3385d2e8f79a7c1e1017/fasteners-0.14.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-resumable-media>=0.2.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (0.3.2)\n",
            "Requirement already satisfied: google-cloud-core<0.30dev,>=0.28.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (0.28.1)\n",
            "Requirement already satisfied: ordereddict in /usr/local/envs/py2env/lib/python2.7/site-packages (from funcsigs>=1->mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.9.0) (1.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (2018.11.29)\n",
            "Requirement already satisfied: google-auth<2.0.0dev,>=0.4.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (1.6.2)\n",
            "Collecting monotonic>=0.1 (from fasteners>=0.14->google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp]==2.9.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-auth<2.0.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (2.1.0)\n",
            "Building wheels for collected packages: tensorflow-transform, httplib2, pyvcf, pydot, hdfs, docopt, grpc-google-iam-v1\n",
            "  Running setup.py bdist_wheel for tensorflow-transform: started\n",
            "  Running setup.py bdist_wheel for tensorflow-transform: finished with status 'done'\n",
            "  Stored in directory: /content/.cache/pip/wheels/e4/ab/1d/d67b14e50932a94c983c54cc9d1d0236ea604114994b4f9ad0\n",
            "  Running setup.py bdist_wheel for httplib2: started\n",
            "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
            "  Stored in directory: /content/.cache/pip/wheels/1b/9c/9e/1f6fdb21dbb1fe6a99101d697f12cb8c1fa96c1587df69adba\n",
            "  Running setup.py bdist_wheel for pyvcf: started\n",
            "  Running setup.py bdist_wheel for pyvcf: finished with status 'done'\n",
            "  Stored in directory: /content/.cache/pip/wheels/81/91/41/3272543c0b9c61da9c525f24ee35bae6fe8f60d4858c66805d\n",
            "  Running setup.py bdist_wheel for pydot: started\n",
            "  Running setup.py bdist_wheel for pydot: finished with status 'done'\n",
            "  Stored in directory: /content/.cache/pip/wheels/6a/a5/14/25541ebcdeaf97a37b6d05c7ff15f5bd20f5e91b99d313e5b4\n",
            "  Running setup.py bdist_wheel for hdfs: started\n",
            "  Running setup.py bdist_wheel for hdfs: finished with status 'done'\n",
            "  Stored in directory: /content/.cache/pip/wheels/1e/43/63/d5848bd5d5e02c449f1112cb23965a16ad97902fe6b53b87da\n",
            "  Running setup.py bdist_wheel for docopt: started\n",
            "  Running setup.py bdist_wheel for docopt: finished with status 'done'\n",
            "  Stored in directory: /content/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
            "  Running setup.py bdist_wheel for grpc-google-iam-v1: started\n",
            "  Running setup.py bdist_wheel for grpc-google-iam-v1: finished with status 'done'\n",
            "  Stored in directory: /content/.cache/pip/wheels/b6/c6/31/c20321a5a3fde456fc375b7c2814135e6e98bc0d74c40239d9\n",
            "Successfully built tensorflow-transform httplib2 pyvcf pydot hdfs docopt grpc-google-iam-v1\n",
            "Installing collected packages: httplib2, typing, pyvcf, pydot, fastavro, docopt, hdfs, grpc-google-iam-v1, google-cloud-pubsub, monotonic, fasteners, google-apitools, google-cloud-bigquery, apache-beam, tensorflow-transform\n",
            "  Found existing installation: httplib2 0.12.0\n",
            "    Uninstalling httplib2-0.12.0:\n",
            "      Successfully uninstalled httplib2-0.12.0\n",
            "  Found existing installation: google-apitools 0.5.10\n",
            "    Uninstalling google-apitools-0.5.10:\n",
            "      Successfully uninstalled google-apitools-0.5.10\n",
            "  Found existing installation: google-cloud-bigquery 0.23.0\n",
            "    Uninstalling google-cloud-bigquery-0.23.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-0.23.0\n",
            "Successfully installed apache-beam-2.9.0 docopt-0.6.2 fastavro-0.21.22 fasteners-0.14.1 google-apitools-0.5.24 google-cloud-bigquery-1.6.1 google-cloud-pubsub-0.35.4 grpc-google-iam-v1-0.11.4 hdfs-2.5.2 httplib2-0.11.3 monotonic-1.5 pydot-1.2.4 pyvcf-0.6.8 tensorflow-transform-0.8.0 typing-3.6.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "google-cloud-bigquery 1.6.1 has requirement google-api-core<2.0.0dev,>=1.0.0, but you'll have google-api-core 0.1.4 which is incompatible.\n",
            "googledatastore 7.0.1 has requirement httplib2<0.10,>=0.9.1, but you'll have httplib2 0.11.3 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q-6qYbW9vB6O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Restart the kernel</b> after you do a pip install (click on the <b>Reset</b> button in Datalab)"
      ]
    },
    {
      "metadata": {
        "id": "nALkCKDDvB6P",
        "colab_type": "code",
        "colab": {},
        "outputId": "2df659c1-199c-42e2-bae7-3554e1231ee7"
      },
      "cell_type": "code",
      "source": [
        "%bash\n",
        "pip freeze | grep -e 'flow\\|beam'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apache-airflow==1.9.0\n",
            "apache-beam==2.9.0\n",
            "tensorflow==1.8.0\n",
            "tensorflow-transform==0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JoDLFmDPvB6U",
        "colab_type": "code",
        "colab": {},
        "outputId": "3b7f70ba-0f66-4679-b03c-72060fe3cc64"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import shutil\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XwWLmR1VvB6Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# change these to try this notebook out\n",
        "BUCKET = 'qwiklabs-gcp-2194bbaf8a54e2ad'\n",
        "PROJECT = 'qwiklabs-gcp-2194bbaf8a54e2ad'\n",
        "REGION = 'asia-southeast1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vHla3B1OvB6a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['REGION'] = REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_7vqsBrpvB6c",
        "colab_type": "code",
        "colab": {},
        "outputId": "b815ecf2-edb4-4fd0-a102-c7bbd073456c"
      },
      "cell_type": "code",
      "source": [
        "%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "x-xtfyuXvB6f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
        "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XYHW4hm7vB6i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Input source: BigQuery\n",
        "\n",
        "Get data from BigQuery but defer filtering etc. to Beam.\n",
        "Note that the dayofweek column is now strings."
      ]
    },
    {
      "metadata": {
        "id": "reigf8FnvB6i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import google.datalab.bigquery as bq\n",
        "def create_query(phase, EVERY_N):\n",
        "  \"\"\"\n",
        "  phase: 1=train 2=valid\n",
        "  \"\"\"\n",
        "  base_query = \"\"\"\n",
        "WITH daynames AS\n",
        "  (SELECT ['Sun', 'Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat'] AS daysofweek)\n",
        "SELECT\n",
        "  (tolls_amount + fare_amount) AS fare_amount,\n",
        "  daysofweek[ORDINAL(EXTRACT(DAYOFWEEK FROM pickup_datetime))] AS dayofweek,\n",
        "  EXTRACT(HOUR FROM pickup_datetime) AS hourofday,\n",
        "  pickup_longitude AS pickuplon,\n",
        "  pickup_latitude AS pickuplat,\n",
        "  dropoff_longitude AS dropofflon,\n",
        "  dropoff_latitude AS dropofflat,\n",
        "  passenger_count AS passengers,\n",
        "  'notneeded' AS key\n",
        "FROM\n",
        "  `nyc-tlc.yellow.trips`, daynames\n",
        "WHERE\n",
        "  trip_distance > 0 AND fare_amount > 0\n",
        "  \"\"\"\n",
        "\n",
        "  if EVERY_N == None:\n",
        "    if phase < 2:\n",
        "      # training\n",
        "      query = \"{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)),4) < 2\".format(base_query)\n",
        "    else:\n",
        "      query = \"{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)),4) = {1}\".format(base_query, phase)\n",
        "  else:\n",
        "      query = \"{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))),{1}) = {2}\".format(base_query, EVERY_N, phase)\n",
        "    \n",
        "  return query\n",
        "\n",
        "query = create_query(2, 100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXSZy6JMvB6m",
        "colab_type": "code",
        "colab": {},
        "outputId": "84802b43-2178-46f7-8044-b0e303dc091a"
      },
      "cell_type": "code",
      "source": [
        "df_valid = bq.Query(query).execute().result().to_dataframe()\n",
        "display(df_valid.head())\n",
        "df_valid.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>dayofweek</th>\n",
              "      <th>hourofday</th>\n",
              "      <th>pickuplon</th>\n",
              "      <th>pickuplat</th>\n",
              "      <th>dropofflon</th>\n",
              "      <th>dropofflat</th>\n",
              "      <th>passengers</th>\n",
              "      <th>key</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>Sun</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.990792</td>\n",
              "      <td>40.686295</td>\n",
              "      <td>-73.996470</td>\n",
              "      <td>40.686182</td>\n",
              "      <td>1</td>\n",
              "      <td>notneeded</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.5</td>\n",
              "      <td>Sun</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.995475</td>\n",
              "      <td>40.717172</td>\n",
              "      <td>-73.990305</td>\n",
              "      <td>40.723332</td>\n",
              "      <td>1</td>\n",
              "      <td>notneeded</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11.0</td>\n",
              "      <td>Sun</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.979865</td>\n",
              "      <td>40.740787</td>\n",
              "      <td>-73.987555</td>\n",
              "      <td>40.722502</td>\n",
              "      <td>1</td>\n",
              "      <td>notneeded</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12.1</td>\n",
              "      <td>Thurs</td>\n",
              "      <td>0</td>\n",
              "      <td>-74.009379</td>\n",
              "      <td>40.704176</td>\n",
              "      <td>-73.987394</td>\n",
              "      <td>40.732475</td>\n",
              "      <td>1</td>\n",
              "      <td>notneeded</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.0</td>\n",
              "      <td>Sun</td>\n",
              "      <td>0</td>\n",
              "      <td>-73.987910</td>\n",
              "      <td>40.738000</td>\n",
              "      <td>-73.989200</td>\n",
              "      <td>40.743850</td>\n",
              "      <td>1</td>\n",
              "      <td>notneeded</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fare_amount dayofweek  hourofday  pickuplon  pickuplat  dropofflon  \\\n",
              "0          4.0       Sun          0 -73.990792  40.686295  -73.996470   \n",
              "1          7.5       Sun          0 -73.995475  40.717172  -73.990305   \n",
              "2         11.0       Sun          0 -73.979865  40.740787  -73.987555   \n",
              "3         12.1     Thurs          0 -74.009379  40.704176  -73.987394   \n",
              "4          6.0       Sun          0 -73.987910  40.738000  -73.989200   \n",
              "\n",
              "   dropofflat  passengers        key  \n",
              "0   40.686182           1  notneeded  \n",
              "1   40.723332           1  notneeded  \n",
              "2   40.722502           1  notneeded  \n",
              "3   40.732475           1  notneeded  \n",
              "4   40.743850           1  notneeded  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>hourofday</th>\n",
              "      <th>pickuplon</th>\n",
              "      <th>pickuplat</th>\n",
              "      <th>dropofflon</th>\n",
              "      <th>dropofflat</th>\n",
              "      <th>passengers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>11181.000000</td>\n",
              "      <td>11181.000000</td>\n",
              "      <td>11181.000000</td>\n",
              "      <td>11181.000000</td>\n",
              "      <td>11181.000000</td>\n",
              "      <td>11181.000000</td>\n",
              "      <td>11181.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>11.242599</td>\n",
              "      <td>13.244075</td>\n",
              "      <td>-72.576852</td>\n",
              "      <td>39.973146</td>\n",
              "      <td>-72.748974</td>\n",
              "      <td>40.006091</td>\n",
              "      <td>1.722118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.447462</td>\n",
              "      <td>6.548354</td>\n",
              "      <td>10.133452</td>\n",
              "      <td>5.777329</td>\n",
              "      <td>12.981577</td>\n",
              "      <td>5.664887</td>\n",
              "      <td>1.351062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-78.133333</td>\n",
              "      <td>-73.991278</td>\n",
              "      <td>-751.400000</td>\n",
              "      <td>-73.977970</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>-73.991849</td>\n",
              "      <td>40.734954</td>\n",
              "      <td>-73.991236</td>\n",
              "      <td>40.734008</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.500000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>-73.981824</td>\n",
              "      <td>40.752640</td>\n",
              "      <td>-73.980164</td>\n",
              "      <td>40.753427</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>12.500000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>-73.967418</td>\n",
              "      <td>40.766700</td>\n",
              "      <td>-73.964153</td>\n",
              "      <td>40.767832</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>143.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>40.806487</td>\n",
              "      <td>41.366138</td>\n",
              "      <td>40.785400</td>\n",
              "      <td>41.366138</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        fare_amount     hourofday     pickuplon     pickuplat    dropofflon  \\\n",
              "count  11181.000000  11181.000000  11181.000000  11181.000000  11181.000000   \n",
              "mean      11.242599     13.244075    -72.576852     39.973146    -72.748974   \n",
              "std        9.447462      6.548354     10.133452      5.777329     12.981577   \n",
              "min        2.500000      0.000000    -78.133333    -73.991278   -751.400000   \n",
              "25%        6.000000      9.000000    -73.991849     40.734954    -73.991236   \n",
              "50%        8.500000     14.000000    -73.981824     40.752640    -73.980164   \n",
              "75%       12.500000     19.000000    -73.967418     40.766700    -73.964153   \n",
              "max      143.000000     23.000000     40.806487     41.366138     40.785400   \n",
              "\n",
              "         dropofflat    passengers  \n",
              "count  11181.000000  11181.000000  \n",
              "mean      40.006091      1.722118  \n",
              "std        5.664887      1.351062  \n",
              "min      -73.977970      0.000000  \n",
              "25%       40.734008      1.000000  \n",
              "50%       40.753427      1.000000  \n",
              "75%       40.767832      2.000000  \n",
              "max       41.366138      6.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "1_iBWJTxvB6q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create ML dataset using tf.transform and Dataflow\n",
        "\n",
        "Let's use Cloud Dataflow to read in the BigQuery data and write it out as CSV files. Along the way, let's use tf.transform to do scaling and transforming. Using tf.transform allows us to save the metadata to ensure that the appropriate transformations get carried out during prediction as well."
      ]
    },
    {
      "metadata": {
        "id": "YbCbrVgrvB6q",
        "colab_type": "code",
        "colab": {},
        "outputId": "97081375-7132-4d18-e5aa-b8a3a0373347"
      },
      "cell_type": "code",
      "source": [
        "%writefile requirements.txt\n",
        "tensorflow-transform==0.8.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IJk2WZM2vB6u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test transform_data is type pcollection. test if _ = is neccesary"
      ]
    },
    {
      "metadata": {
        "id": "GlUHGbCvvB6v",
        "colab_type": "code",
        "colab": {},
        "outputId": "f137cb5c-2e6e-4529-f6b1-b52526422344"
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import tensorflow as tf\n",
        "import apache_beam as beam\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.beam import impl as beam_impl\n",
        "\n",
        "def is_valid(inputs):\n",
        "  try:\n",
        "    pickup_longitude = inputs['pickuplon']\n",
        "    dropoff_longitude = inputs['dropofflon']\n",
        "    pickup_latitude = inputs['pickuplat']\n",
        "    dropoff_latitude = inputs['dropofflat']\n",
        "    hourofday = inputs['hourofday']\n",
        "    dayofweek = inputs['dayofweek']\n",
        "    passenger_count = inputs['passengers']\n",
        "    fare_amount = inputs['fare_amount']\n",
        "    return (fare_amount >= 2.5 and pickup_longitude > -78 and pickup_longitude < -70 \\\n",
        "      and dropoff_longitude > -78 and dropoff_longitude < -70 and pickup_latitude > 37 \\\n",
        "      and pickup_latitude < 45 and dropoff_latitude > 37 and dropoff_latitude < 45 \\\n",
        "      and passenger_count > 0)\n",
        "  except:\n",
        "    return False\n",
        "  \n",
        "def preprocess_tft(inputs):\n",
        "      import datetime   \n",
        "      print inputs\n",
        "      result = {}\n",
        "      result['fare_amount'] = tf.identity(inputs['fare_amount'])     \n",
        "      result['dayofweek'] = tft.string_to_int(inputs['dayofweek']) # builds a vocabulary\n",
        "      result['hourofday'] = tf.identity(inputs['hourofday']) # pass through\n",
        "      result['pickuplon'] = (tft.scale_to_0_1(inputs['pickuplon'])) # scaling numeric values\n",
        "      result['pickuplat'] = (tft.scale_to_0_1(inputs['pickuplat']))\n",
        "      result['dropofflon'] = (tft.scale_to_0_1(inputs['dropofflon']))\n",
        "      result['dropofflat'] = (tft.scale_to_0_1(inputs['dropofflat']))\n",
        "      result['passengers'] = tf.cast(inputs['passengers'], tf.float32) # a cast\n",
        "      result['key'] = tf.as_string(tf.ones_like(inputs['passengers'])) # arbitrary TF func\n",
        "      # engineered features\n",
        "      latdiff = inputs['pickuplat'] - inputs['dropofflat']\n",
        "      londiff = inputs['pickuplon'] - inputs['dropofflon']\n",
        "      result['latdiff'] = tft.scale_to_0_1(latdiff)\n",
        "      result['londiff'] = tft.scale_to_0_1(londiff)\n",
        "      dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
        "      result['euclidean'] = tft.scale_to_0_1(dist)\n",
        "      return result\n",
        "\n",
        "def preprocess(in_test_mode):\n",
        "  import os\n",
        "  import os.path\n",
        "  import tempfile\n",
        "  from apache_beam.io import tfrecordio\n",
        "  from tensorflow_transform.coders import example_proto_coder\n",
        "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "  from tensorflow_transform.tf_metadata import dataset_schema\n",
        "  from tensorflow_transform.beam import tft_beam_io\n",
        "  from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
        "\n",
        "  job_name = 'preprocess-taxi-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
        "  if in_test_mode:\n",
        "    import shutil\n",
        "    print 'Launching local job ... hang on'\n",
        "    OUTPUT_DIR = './preproc_tft'\n",
        "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
        "    EVERY_N = 100000\n",
        "  else:\n",
        "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
        "    OUTPUT_DIR = 'gs://{0}/taxifare/preproc_tft/'.format(BUCKET)\n",
        "    import subprocess\n",
        "    subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
        "    EVERY_N = 10000\n",
        "    \n",
        "  options = {\n",
        "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
        "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
        "    'job_name': job_name,\n",
        "    'project': PROJECT,\n",
        "    'max_num_workers': 16,\n",
        "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
        "    'no_save_main_session': True,\n",
        "    'requirements_file': 'requirements.txt'\n",
        "  }\n",
        "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
        "  if in_test_mode:\n",
        "    RUNNER = 'DirectRunner'\n",
        "  else:\n",
        "    RUNNER = 'DataflowRunner'\n",
        "\n",
        "  # set up raw data metadata\n",
        "  raw_data_schema = {\n",
        "    colname : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
        "                   for colname in 'dayofweek,key'.split(',')\n",
        "  }\n",
        "  raw_data_schema.update({\n",
        "      colname : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
        "                   for colname in 'fare_amount,pickuplon,pickuplat,dropofflon,dropofflat'.split(',')\n",
        "    })\n",
        "  raw_data_schema.update({\n",
        "      colname : dataset_schema.ColumnSchema(tf.int64, [], dataset_schema.FixedColumnRepresentation())\n",
        "                   for colname in 'hourofday,passengers'.split(',')\n",
        "    })\n",
        "  raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
        "\n",
        "  # run Beam  \n",
        "  with beam.Pipeline(RUNNER, options=opts) as p:\n",
        "    with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
        "      # save the raw data metadata\n",
        "      raw_data_metadata | 'WriteInputMetadata' >> tft_beam_io.WriteMetadata(\n",
        "            os.path.join(OUTPUT_DIR, 'metadata/rawdata_metadata'),\n",
        "            pipeline=p)\n",
        "      \n",
        "      # read training data from bigquery and filter rows     \n",
        "      raw_data = (p \n",
        "        | 'train_read' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(1, EVERY_N), use_standard_sql=True))\n",
        "        | 'train_filter' >> beam.Filter(is_valid))\n",
        "      raw_dataset = (raw_data, raw_data_metadata)\n",
        "      \n",
        "      # analyze and transform training data\n",
        "      transformed_dataset, transform_fn = (\n",
        "          raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))\n",
        "      transformed_data, transformed_metadata = transformed_dataset\n",
        "      \n",
        "      # save transformed training data to disk in efficient tfrecord format\n",
        "      transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
        "          os.path.join(OUTPUT_DIR, 'train'),\n",
        "          file_name_suffix='.gz',\n",
        "          coder=example_proto_coder.ExampleProtoCoder(\n",
        "              transformed_metadata.schema))\n",
        "      \n",
        "      # read eval data from bigquery and filter rows  \n",
        "      raw_test_data = (p \n",
        "        | 'eval_read' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(2, EVERY_N), use_standard_sql=True))\n",
        "        | 'eval_filter' >> beam.Filter(is_valid))\n",
        "      raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
        "      \n",
        "      # transform eval data\n",
        "      transformed_test_dataset = (\n",
        "          (raw_test_dataset, transform_fn) | beam_impl.TransformDataset())\n",
        "      transformed_test_data, _ = transformed_test_dataset\n",
        "      \n",
        "      # save transformed training data to disk in efficient tfrecord format\n",
        "      transformed_test_data | 'WriteTestData' >> tfrecordio.WriteToTFRecord(\n",
        "          os.path.join(OUTPUT_DIR, 'eval'),\n",
        "          file_name_suffix='.gz',\n",
        "          coder=example_proto_coder.ExampleProtoCoder(\n",
        "              transformed_metadata.schema))\n",
        "      \n",
        "      # save transformation function to disk for use at serving time\n",
        "      transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(\n",
        "          os.path.join(OUTPUT_DIR, 'metadata'))\n",
        "\n",
        "preprocess(in_test_mode=False) # change to True to run locally"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Launching Dataflow job preprocess-taxi-features-190501-072321 ... hang on\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:800: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
            "  options = pbegin.pipeline.options.view_as(DebugOptions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'dayofweek': <tf.Tensor 'inputs/dayofweek_copy:0' shape=(?,) dtype=string>, 'passengers': <tf.Tensor 'inputs/passengers_copy:0' shape=(?,) dtype=int64>, 'fare_amount': <tf.Tensor 'inputs/fare_amount_copy:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'inputs/pickuplat_copy:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'inputs/dropofflat_copy:0' shape=(?,) dtype=float32>, 'key': <tf.Tensor 'inputs/key_copy:0' shape=(?,) dtype=string>, 'hourofday': <tf.Tensor 'inputs/hourofday_copy:0' shape=(?,) dtype=int64>, 'pickuplon': <tf.Tensor 'inputs/pickuplon_copy:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'inputs/dropofflon_copy:0' shape=(?,) dtype=float32>}\n",
            "WARNING:tensorflow:From <ipython-input-11-6186b4b87796>:26: string_to_int (from tensorflow_transform.mappers) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tft.compute_and_apply_vocabulary()` instead.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: gs://qwiklabs-gcp-2194bbaf8a54e2ad/taxifare/preproc_tft/tmp/tftransform_tmp/70a36fe80ca54b83946ab3a540c7c02e/saved_model.pb\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: gs://qwiklabs-gcp-2194bbaf8a54e2ad/taxifare/preproc_tft/tmp/tftransform_tmp/7773f2861b954ffeb5d2e3076d5424a7/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/envs/py2env/lib/python2.7/site-packages/oauth2client/contrib/gce.py:99: UserWarning: You have requested explicit scopes to be used with a GCE service account.\n",
            "Using this argument will have no effect on the actual scopes for tokens\n",
            "requested. These scopes are set at VM instance creation time and\n",
            "can't be overridden in the request.\n",
            "\n",
            "  warnings.warn(_SCOPES_WARNING)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ContextualVersionConflict",
          "evalue": "(httplib2 0.12.0 (/usr/local/envs/py2env/lib/python2.7/site-packages), Requirement.parse('httplib2<=0.11.3,>=0.8'), set(['apache-beam']))",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mContextualVersionConflict\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6186b4b87796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m           os.path.join(OUTPUT_DIR, 'metadata'))\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_test_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# change to True to run locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-6186b4b87796>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(in_test_mode)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0;31m# save transformation function to disk for use at serving time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(\n\u001b[0;32m--> 145\u001b[0;31m           os.path.join(OUTPUT_DIR, 'metadata'))\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_test_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# change to True to run locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m           self._options).run(False)\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTypeOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime_type_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.pyc\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# raise an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     result = DataflowPipelineResult(\n\u001b[0;32m--> 394\u001b[0;31m         self.dataflow_client.create_job(self.job), self)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;31m# TODO(BEAM-4274): Circular import runners-metrics. Requires refactoring.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/utils/retry.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36mcreate_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    498\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;34m\"\"\"Creates job description. May stage and/or submit for remote execution.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_job_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;31m# Stage and submit the job when necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36mcreate_job_description\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;31m# Stage other resources for the SDK harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m     \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stage_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     job.proto.environment = Environment(\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36m_stage_resources\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mtemp_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         staging_location=google_cloud_options.staging_location)\n\u001b[0m\u001b[1;32m    463\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/portability/stager.pyc\u001b[0m in \u001b[0;36mstage_job_resources\u001b[0;34m(self, options, build_setup_args, temp_dir, populate_requirements_cache, staging_location)\u001b[0m\n\u001b[1;32m    226\u001b[0m         resources.extend(\n\u001b[1;32m    227\u001b[0m             self._stage_beam_sdk(sdk_remote_location, staging_location,\n\u001b[0;32m--> 228\u001b[0;31m                                  temp_dir))\n\u001b[0m\u001b[1;32m    229\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0msetup_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Use the SDK that's built into the container, rather than re-staging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/portability/stager.pyc\u001b[0m in \u001b[0;36m_stage_beam_sdk\u001b[0;34m(self, sdk_remote_location, staging_location, temp_dir)\u001b[0m\n\u001b[1;32m    479\u001b[0m       \"\"\"\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msdk_remote_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pypi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m       \u001b[0msdk_local_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_pypi_sdk_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m       \u001b[0msdk_sources_staged_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m           \u001b[0m_desired_sdk_filename_in_staging_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdk_local_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/portability/stager.pyc\u001b[0m in \u001b[0;36m_download_pypi_sdk_package\u001b[0;34m(temp_dir, fetch_binary, language_version_tag, language_implementation_tag, abi_tag, platform_tag)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mpackage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sdk_package_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       raise RuntimeError('Please set --sdk_location command-line option '\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \"\"\"\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mContextualVersionConflict\u001b[0m: (httplib2 0.12.0 (/usr/local/envs/py2env/lib/python2.7/site-packages), Requirement.parse('httplib2<=0.11.3,>=0.8'), set(['apache-beam']))"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "bIkMfmNXvB6y",
        "colab_type": "code",
        "colab": {},
        "outputId": "e77d6469-9af5-4390-89a6-d5cbeca0edd7"
      },
      "cell_type": "code",
      "source": [
        "%bash\n",
        "# ls preproc_tft\n",
        "gsutil ls gs://${BUCKET}/taxifare/preproc_tft/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://qwiklabs-gcp-2194bbaf8a54e2ad/taxifare/preproc_tft/\n",
            "gs://qwiklabs-gcp-2194bbaf8a54e2ad/taxifare/preproc_tft/tmp/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cGf-TdZvvB61",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Train off preprocessed data </h2>"
      ]
    },
    {
      "metadata": {
        "id": "PyhaeCEivB62",
        "colab_type": "code",
        "colab": {},
        "outputId": "e42cf575-3f8c-4b2f-a50d-c5edcfa4c942"
      },
      "cell_type": "code",
      "source": [
        "%bash\n",
        "rm -rf taxifare_tft.tar.gz taxi_trained\n",
        "export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
        "python -m trainer.task \\\n",
        "   --train_data_paths=\"gs://${BUCKET}/taxifare/preproc_tft/train*\" \\\n",
        "   --eval_data_paths=\"gs://${BUCKET}/taxifare/preproc_tft/eval*\"  \\\n",
        "   --output_dir=./taxi_trained \\\n",
        "   --train_steps=10 --job-dir=/tmp \\\n",
        "   --metadata_path=gs://${BUCKET}/taxifare/preproc_tft/metadata"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8a1db9b0d0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': './taxi_trained', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/datalab/training-data-analyst/courses/machine_learning/feateng/taxifare_tft/trainer/task.py\", line 127, in <module>\n",
            "    model.train_and_evaluate(arguments)\n",
            "  File \"/content/datalab/training-data-analyst/courses/machine_learning/feateng/taxifare_tft/trainer/model.py\", line 168, in train_and_evaluate\n",
            "    input_fn = read_dataset(args, tf.estimator.ModeKeys.TRAIN),\n",
            "  File \"/content/datalab/training-data-analyst/courses/machine_learning/feateng/taxifare_tft/trainer/model.py\", line 150, in read_dataset\n",
            "    os.path.join(args['metadata_path'], 'transformed_metadata'))\n",
            "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow_transform/tf_metadata/metadata_io.py\", line 40, in read_metadata\n",
            "    _read_merge(dm, paths, versions)\n",
            "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow_transform/tf_metadata/metadata_io.py\", line 88, in _read_merge\n",
            "    other = version.read(vdir)\n",
            "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow_transform/tf_metadata/version_api.py\", line 59, in read\n",
            "    vdir.schema_filename)\n",
            "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow_transform/tf_metadata/v1_json/schema_io_v1_json.py\", line 54, in read\n",
            "    raise IOError(\"v1 Schema file does not exist at: %s\" % path)\n",
            "IOError: v1 Schema file does not exist at: gs://qwiklabs-gcp-2194bbaf8a54e2ad/taxifare/preproc_tft/metadata/transformed_metadata/v1-json/schema\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XhxIZ1h6vB68",
        "colab_type": "code",
        "colab": {},
        "outputId": "c90b1546-3264-40ed-cf5b-0f0f283a6da0"
      },
      "cell_type": "code",
      "source": [
        "!ls $PWD/taxi_trained/export/exporter"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/datalab/training-data-analyst/courses/machine_learning/feateng/taxi_trained/export/exporter': No such file or directory\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z4xsaHRsvB7A",
        "colab_type": "code",
        "colab": {},
        "outputId": "074ee31f-4266-44d1-cecd-ca7c22f2779a"
      },
      "cell_type": "code",
      "source": [
        "%writefile /tmp/test.json\n",
        "{\"dayofweek\":\"Thu\",\"hourofday\":17,\"pickuplon\": -73.885262,\"pickuplat\": 40.773008,\"dropofflon\": -73.987232,\"dropofflat\": 40.732403,\"passengers\": 2}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /tmp/test.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kMO3a1smvB7E",
        "colab_type": "code",
        "colab": {},
        "outputId": "094e4586-5748-475b-b5a0-6a39369fe203"
      },
      "cell_type": "code",
      "source": [
        "%bash\n",
        "model_dir=$(ls $PWD/taxi_trained/export/exporter/)\n",
        "gcloud ml-engine local predict \\\n",
        "    --model-dir=./taxi_trained/export/exporter/${model_dir} \\\n",
        "    --json-instances=/tmp/test.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/datalab/training-data-analyst/courses/machine_learning/feateng/taxi_trained/export/exporter/': No such file or directory\n",
            "ERROR: (gcloud.ml-engine.local.predict) /usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Traceback (most recent call last):\n",
            "  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 184, in <module>\n",
            "    main()\n",
            "  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 179, in main\n",
            "    signature_name=args.signature_name)\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_lib.py\", line 98, in local_predict\n",
            "    client = create_client(framework, model_dir, **kwargs)\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_lib.py\", line 91, in create_client\n",
            "    return create_client_fn(model_path, **kwargs)\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py\", line 555, in create_tf_session_client\n",
            "    return SessionClient(*load_tf_model(model_dir, tags, config))\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py\", line 218, in load_tf_model\n",
            "    \"Cloud ML only supports TF 1.0 or above and models \"\n",
            "cloud.ml.prediction.prediction_utils.PredictionError: Failed to load model: Cloud ML only supports TF 1.0 or above and models saved in SavedModel format. (Error code: 0)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "buoHf5uQvB7J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2016-2018 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
      ]
    }
  ]
}